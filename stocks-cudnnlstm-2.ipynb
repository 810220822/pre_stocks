{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46697404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:25.410305Z",
     "iopub.status.busy": "2023-03-05T21:03:25.409662Z",
     "iopub.status.idle": "2023-03-05T21:03:42.503279Z",
     "shell.execute_reply": "2023-03-05T21:03:42.501763Z"
    },
    "papermill": {
     "duration": 17.104779,
     "end_time": "2023-03-05T21:03:42.506369",
     "exception": false,
     "start_time": "2023-03-05T21:03:25.401590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler # pip3 install --upgrade --force-reinstall scikit-learn --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential #pip3 install --upgrade --force-reinstall keras --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
    "from tensorflow.keras.models import load_model #pip3 install --upgrade --force-reinstall keras --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
    "from tensorflow.keras.layers import LSTM,Dense,Dropout\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,Callback,CSVLogger,ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.utils import multi_gpu_utils\n",
    "import os\n",
    "from io import StringIO\n",
    "import gzip\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from shutil import copyfile\n",
    "# copy our file into the working directory (make sure it has .py suffix)\n",
    "copyfile(src = \"/kaggle/input/stocks-code/stocks.py\", dst = \"../working/stocks.py\")\n",
    " \n",
    "# import all our functions\n",
    "from stocks import stocks_all\n",
    "from stocks import bankuai\n",
    "\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "threads = []\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #消除tensorflow警告\n",
    "\n",
    "model_saved_log_char = datetime.datetime.now().strftime('%Y%m%d%h%m%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01b2556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.519487Z",
     "iopub.status.busy": "2023-03-05T21:03:42.517368Z",
     "iopub.status.idle": "2023-03-05T21:03:42.526197Z",
     "shell.execute_reply": "2023-03-05T21:03:42.525082Z"
    },
    "papermill": {
     "duration": 0.017591,
     "end_time": "2023-03-05T21:03:42.528902",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.511311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#获取数据\n",
    "start = datetime.datetime(2000,1,1)\n",
    "end =  datetime.date.today()\n",
    "\n",
    "#参数整理\n",
    "EarlyStopping_monitor='val_loss' #monitor——被监测的量\n",
    "EarlyStopping_patience=10 #检测值停止变化的次数\n",
    "\n",
    "_mem_days=[1,3,5] #滑动区间，根据几天的数据做预测\n",
    "_lstm_layers,_dense_layers=[1,5],[1,5] #图层数\n",
    "# 这里我们设置的units=32的大小，其实代表得是LSTM单元内的隐藏层的尺寸。\n",
    "# 对于LSTM而言，每个单元有3个门，对应了4个激活函数（3个sigmoid,一个tanh）。也就是说有4个神经元数量为32的前馈网络层。\n",
    "_units= [32,64]\n",
    "\n",
    "# #测试\n",
    "# _mem_days=[3] #滑动区间，根据几天的数据做预测\n",
    "# _lstm_layers,_dense_layers=[1],[1] #图层数\n",
    "# _units= [32]\n",
    "\n",
    "\n",
    "optimizer='adam' #优化器:控制梯度下降和梯度爆炸\n",
    "loss = 'mse' #损失层\n",
    "metrics=['mape'] #评价函数\n",
    "batch_size=32 #每次训练在训练集中取batchsize个样本训练；.batch_size=1时为在线学习，也是标准的SGD,如果数据集比较小，则完全可以采用全数据集的形式;GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优\n",
    "epochs=50 #一个 epoch（代）是指整个数据集正向反向训练一次。\n",
    "\n",
    "model_verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9b97d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.540404Z",
     "iopub.status.busy": "2023-03-05T21:03:42.539714Z",
     "iopub.status.idle": "2023-03-05T21:03:42.548617Z",
     "shell.execute_reply": "2023-03-05T21:03:42.547415Z"
    },
    "papermill": {
     "duration": 0.017587,
     "end_time": "2023-03-05T21:03:42.551273",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.533686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#文件路径 data_\n",
    "# path = '/kaggle/input/stocks-data-20221216/'\n",
    "log_file_name = '/kaggle/working/models'\n",
    "model_saved_file='/kaggle/working/models_2'\n",
    "BASE_PATH = '/kaggle/input/data03/data03'\n",
    "\n",
    "model_saved_log = f'/kaggle/working/models_2/{ model_saved_log_char}_models.csv'\n",
    "\n",
    "[os.makedirs(f\"{log_file_name}/{klt}\", exist_ok=True) for klt in [101, 102, 103]]\n",
    "[os.makedirs(f\"{model_saved_file}/{klt}\", exist_ok=True) for klt in [101, 102, 103]]\n",
    "\n",
    "# #创建任务总模型目录\n",
    "log_csv_file = open(model_saved_log, 'a')\n",
    "\n",
    "# 写表头code,loss,mape,val_loss,val_mape,modelname\n",
    "model_log = f'code,klt,loss,mape,val_loss,val_mape,modelname\\n'\n",
    "log_csv_file.write(model_log)\n",
    "log_csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f71dada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.562879Z",
     "iopub.status.busy": "2023-03-05T21:03:42.561820Z",
     "iopub.status.idle": "2023-03-05T21:03:42.568687Z",
     "shell.execute_reply": "2023-03-05T21:03:42.567605Z"
    },
    "papermill": {
     "duration": 0.015027,
     "end_time": "2023-03-05T21:03:42.571112",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.556085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exception_file_full_name = f'/kaggle/working/models_2/{ model_saved_log_char}_exception.txt'\n",
    "\n",
    "#创建异常文件\n",
    "exception_file = open(exception_file_full_name, 'a')\n",
    "\n",
    "# 写表头code,loss,mape,val_loss,val_mape,modelname\n",
    "exception_log = f'---------------Exception:{str(end)}------------------\\n'\n",
    "exception_file.write(exception_log)\n",
    "exception_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae49b8ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.582441Z",
     "iopub.status.busy": "2023-03-05T21:03:42.582080Z",
     "iopub.status.idle": "2023-03-05T21:03:42.603593Z",
     "shell.execute_reply": "2023-03-05T21:03:42.602225Z"
    },
    "papermill": {
     "duration": 0.03117,
     "end_time": "2023-03-05T21:03:42.607060",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.575890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#模型callback类\n",
    "class CustomCallback(Callback):\n",
    "#     print('-----------------CustomCallback-----------------')\n",
    "    code = ''\n",
    "    the_mem_days=0\n",
    "    the_lstm_layers=0\n",
    "    the_dense_layers=0\n",
    "    the_units = 0\n",
    "    csv_file_name = ''\n",
    "    model_path = ''\n",
    "    saveModelFile = False\n",
    "    saveModelLog = True\n",
    "\n",
    "    #epoch,loss,mape,val_loss,val_mape,code,the_mem_days,the_lstm_layers,the_dense_layers,the_units\n",
    "    csv_file = DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,path,csv_file_name,code,the_mem_days,the_lstm_layers,the_dense_layers,the_units,\n",
    "                 saveModelFile=False,saveModelLog=True,klt=101):\n",
    "        self.model_path = path\n",
    "        self.csv_file_name = csv_file_name\n",
    "        self.code = code\n",
    "        self.the_mem_days = the_mem_days\n",
    "        self.the_lstm_layers = the_lstm_layers\n",
    "        self.the_dense_layers = the_dense_layers\n",
    "        self.the_units = the_units\n",
    "        self.saveModelFile = saveModelFile\n",
    "        self.saveModelLog=saveModelLog\n",
    "        self.klt = klt\n",
    "#         print('-----------------CustomCallback__init__-----------------')\n",
    "        #\n",
    "        if not os.path.exists(csv_file_name):\n",
    "#             print('-----------------os.path.exists(csv_file_name)-----------------')\n",
    "            # #创建任务总模型目录\n",
    "            _temp_file = open(csv_file_name, 'a') \n",
    "            _temp_file_header = f'epoch,loss,mape,val_loss,val_mape,code,klt,the_mem_days,the_lstm_layers,the_dense_layers,the_units\\n'\n",
    "            _temp_file.write(_temp_file_header)\n",
    "            _temp_file.close()\n",
    "#         print('-----------------self.csv_file-----------------')\n",
    "        self.csv_file = pd.read_csv(csv_file_name, lineterminator='\\n', header=0)  \n",
    "                \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "#         print('-----------------self.on_epoch_end-----------------')\n",
    "        if self.saveModelFile == True:\n",
    "#             print('-----------------self.saveModelFile-----------------')\n",
    "            loss = logs['loss']\n",
    "            filepath =  f'{self.model_path}/{self.klt}/{loss:.2f}_{self.code}_{epoch:02}_mem_{self.the_mem_days}_ltsm_{self.the_lstm_layers}_dense_{self.the_dense_layers}_unit_{self.the_units}.h5'\n",
    "\n",
    "            loss = logs['loss']\n",
    "            mape = logs['mape']\n",
    "            val_loss = logs['val_loss']\n",
    "            val_mape = logs['val_mape']\n",
    "            log_csv_file = open(model_saved_log, 'a+')\n",
    "            # code,loss,mape,val_loss,val_mape,modelname\n",
    "            model_log = f'c{self.code},{self.klt},{loss:.2f},{mape:.2f},{val_loss:.2f},{val_mape:.2f},{filepath}\\n'\n",
    "            log_csv_file.write(model_log)\n",
    "            log_csv_file.close()\n",
    "\n",
    "\n",
    "            self.model.save(filepath,save_format='h5')\n",
    "\n",
    "        if self.saveModelLog == True:\n",
    "#             print('-----------------self.saveModelFile-----------------')\n",
    "            if not math.isnan(logs['loss']) :\n",
    "#                 print('-----------------self.isnan-----------------')\n",
    "                _i_ = len(self.csv_file)\n",
    "                row = {\n",
    "                    'epoch':epoch,\n",
    "                    'loss' : float(round(logs['loss'],2) ),\n",
    "                    'mape':round(logs['mape'],2)  ,\n",
    "                    'val_loss': round(logs['val_loss'],2) ,\n",
    "                    'val_mape': round(logs['val_mape'],2)  ,\n",
    "\n",
    "                    'code': self.code,\n",
    "                    'klt':self.klt,\n",
    "                    'the_mem_days': self.the_mem_days,\n",
    "                    'the_lstm_layers': self.the_lstm_layers,\n",
    "                    'the_dense_layers': self.the_dense_layers,\n",
    "                    'the_units': self.the_units\n",
    "                }\n",
    "\n",
    "                row_index = len(self.csv_file)\n",
    "                self.csv_file.loc[row_index] = row\n",
    "\n",
    "                self.csv_file.to_csv(self.csv_file_name,index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbe8645e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.618538Z",
     "iopub.status.busy": "2023-03-05T21:03:42.618138Z",
     "iopub.status.idle": "2023-03-05T21:03:42.624929Z",
     "shell.execute_reply": "2023-03-05T21:03:42.623648Z"
    },
    "papermill": {
     "duration": 0.015299,
     "end_time": "2023-03-05T21:03:42.627351",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.612052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_dataframe(path):\n",
    "    os_file = open(path, 'rb')  # 打开压缩文件对象\n",
    "    file_stream = os_file.read()\n",
    "    message = gzip.decompress(file_stream).decode('GBK')\n",
    "\n",
    "    dataframe = pd.read_csv(StringIO(message), lineterminator='\\n', \n",
    "                            header=0,dtype={\"CODE\": str},index_col=\"日期\")\n",
    "    os_file.close()\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae5f5908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.638602Z",
     "iopub.status.busy": "2023-03-05T21:03:42.638219Z",
     "iopub.status.idle": "2023-03-05T21:03:42.648066Z",
     "shell.execute_reply": "2023-03-05T21:03:42.646788Z"
    },
    "papermill": {
     "duration": 0.018355,
     "end_time": "2023-03-05T21:03:42.650527",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.632172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_nane_zero(x):\n",
    "    if x == 'None':\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "#打开数据，数据清洗1\n",
    "def lstm_cleanm_data(file_path,klt):\n",
    "    \n",
    "    data = open_dataframe(file_path)\n",
    "    \n",
    "    data = data.drop(['CODE'], axis=1)\n",
    "#     data = data.drop(['NAME'], axis=1)\n",
    "    data.sort_index(inplace=True,ascending=True)  # 排序\n",
    "    \n",
    "    #判断处理类型\n",
    "    if klt==101:\n",
    "        #(['CODE', '开盘', '收盘', '最高', '最低', '成交量', '成交额', '振幅', '涨跌额', '换手率'])\n",
    "        data = data[data.columns[0:9]]\n",
    "    \n",
    "    # 数据处理\n",
    "    for col in data.columns:\n",
    "        data[col].fillna(0, inplace=True)  # 将数学成绩为空值用0填\n",
    "        data[col] = data[col].map(set_nane_zero)\n",
    "    # 删除close的空值\n",
    "    for col  in ['开盘', '收盘', '最高', '最低', '成交量']:\n",
    "        null_idx = data.loc[(data[col] == 0) | (data[col] == np.nan )|(data[col] == 'None')].index\n",
    "        data = data.drop(null_idx)\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbcfc08a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.662089Z",
     "iopub.status.busy": "2023-03-05T21:03:42.661242Z",
     "iopub.status.idle": "2023-03-05T21:03:42.672820Z",
     "shell.execute_reply": "2023-03-05T21:03:42.671762Z"
    },
    "papermill": {
     "duration": 0.019888,
     "end_time": "2023-03-05T21:03:42.675223",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.655335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#数据处理\n",
    "def stock_price_ltsm_data_processing(f,predays,isPredict:bool = False):\n",
    "    # 1.数据处理\n",
    "    predays = int(predays)\n",
    "    f['labels'] = f['收盘'].shift(-predays) #新增滑动列:f['labels'][predays]行等于f['labels'][0],predays正数向上滑动，负数向下滑动\n",
    "\n",
    "    x_data = f[:].iloc[:, :-1].values  # .values将dataframe转换为array\n",
    "\n",
    "    # 1.数据处理-归一化\n",
    "    scaler = StandardScaler() #数据标准化\n",
    "    sca_x = scaler.fit_transform(x_data) #除最后一例的所有行 把新增的滑动列去掉，再进行归一化\n",
    "\n",
    "    # date_begin = str(x_data[predays,:].index)\n",
    "\n",
    "    # 测试\n",
    "    # x_data = f[-(predays + n):].iloc[:, :-1]\n",
    "    #\n",
    "    # x = []  # 参数x\n",
    "    # y = []  # 输出参数y，用于模型巡礼和结果对比\n",
    "    # for i in range(predays, len(x_data)+1):\n",
    "    #     # 序列长度为19\n",
    "    #     x.append(x_data.iloc[i - predays:i, :])\n",
    "    #     # 标签长度为1\n",
    "    #     if i < len(x_data):\n",
    "    #         y.append(x_data.iloc[i, 0])  # 第i行第0个值，'tclose'\n",
    "    #     else:\n",
    "    #         y.append(np.nan)  # 预测值占位\n",
    "\n",
    "    #测试完\n",
    "    length = len(sca_x)\n",
    "    if isPredict == True:\n",
    "        length += 1\n",
    "\n",
    "    # 1.数据处理-分组，准备对照数据\n",
    "    x = []  # 参数x\n",
    "    y = []  # 输出参数y，用于模型巡礼和结果对比\n",
    "    for i in range(predays,length):\n",
    "        # 序列长度为19\n",
    "        x.append(sca_x[i - predays:i, :])\n",
    "        # 标签长度为1\n",
    "        if i < len(sca_x):\n",
    "            y.append(x_data[i, 0])  # 第i行第0个值，'tclose'\n",
    "        else:\n",
    "            y.append(np.nan)  # 预测值占位\n",
    "\n",
    "    # x_lately = x[predays:] #记录失效数据\n",
    "    # x = x[:predays] #删除失效数据\n",
    "\n",
    "    x,y= np.array(x), np.array(y)\n",
    "    x = np.reshape(x,(x.shape[0],x.shape[1],x.shape[2]))\n",
    "\n",
    "\n",
    "\n",
    "    return  x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8950c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.686168Z",
     "iopub.status.busy": "2023-03-05T21:03:42.685806Z",
     "iopub.status.idle": "2023-03-05T21:03:42.690782Z",
     "shell.execute_reply": "2023-03-05T21:03:42.689605Z"
    },
    "papermill": {
     "duration": 0.013337,
     "end_time": "2023-03-05T21:03:42.693307",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.679970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_time_start = time.time()\n",
    "_time_limit = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a908bf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.704813Z",
     "iopub.status.busy": "2023-03-05T21:03:42.703878Z",
     "iopub.status.idle": "2023-03-05T21:03:42.711554Z",
     "shell.execute_reply": "2023-03-05T21:03:42.710366Z"
    },
    "papermill": {
     "duration": 0.015906,
     "end_time": "2023-03-05T21:03:42.713955",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.698049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    " \n",
    "\n",
    "def file2zip(packagePath, zipPath):\n",
    "    '''\n",
    "  :param packagePath: 文件夹路径\n",
    "  :param zipPath: 压缩包路径\n",
    "  :return:\n",
    "  '''\n",
    "    if os.path.exists(zipPath):\n",
    "        os.remove(zipPath)\n",
    "    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n",
    "    for path, dirNames, fileNames in os.walk(packagePath):\n",
    "        fpath = path.replace(packagePath, '')\n",
    "        for name in fileNames:\n",
    "            fullName = os.path.join(path, name)\n",
    "            name = fpath + '\\\\' + name\n",
    "            zip.write(fullName, name)\n",
    "    zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c4b2b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.725044Z",
     "iopub.status.busy": "2023-03-05T21:03:42.724645Z",
     "iopub.status.idle": "2023-03-05T21:03:42.930922Z",
     "shell.execute_reply": "2023-03-05T21:03:42.929307Z"
    },
    "papermill": {
     "duration": 0.224822,
     "end_time": "2023-03-05T21:03:42.943437",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.718615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#建模\n",
    "def build_models(f,code,mem_days,lstm_layers,dense_layers,units,saveModelFile ,saveModelLog,thread_count,klt ):\n",
    "    \n",
    "    build_models_times = 0\n",
    "\n",
    "    for the_mem_days in mem_days:\n",
    "        new_df = f.copy(deep=True)\n",
    "        x, y = stock_price_ltsm_data_processing(new_df,the_mem_days,False)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, shuffle=False, test_size=0.2, random_state=42)\n",
    "        for the_lstm_layers in lstm_layers:\n",
    "            for the_dense_layers in dense_layers:\n",
    "                for the_units in units:\n",
    "#                     print('-----------------callback-----------------')\n",
    "                    callback = [\n",
    "                        EarlyStopping(monitor=EarlyStopping_monitor, patience=EarlyStopping_patience),\n",
    "                        # CSVLogger(filename, separator=',', append=True),\n",
    "                        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto',\n",
    "                                          min_delta=0.0001, cooldown=0, min_lr=0),\n",
    "                        CustomCallback(f'{model_saved_file}/{klt}',f'{log_file_name}/{klt}/{code}.csv',code,the_mem_days,the_lstm_layers,the_dense_layers,the_units,\n",
    "                                       saveModelFile=saveModelFile,saveModelLog=saveModelLog,klt=klt)\n",
    "                    ]\n",
    "#                     print('-----------------Sequential-----------------')\n",
    "                    #构建神经网络\n",
    "                    model = Sequential()\n",
    "#                     from keras.utils import multi_gpu_utils\n",
    "#                     model = multi_gpu_utils(model, gpus=2)\n",
    "                    model.add(CuDNNLSTM(the_units,input_shape=x.shape[1:],return_sequences=True)) #第一层\n",
    "                    model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    for i in range(the_lstm_layers):\n",
    "                        model.add(CuDNNLSTM(the_units,return_sequences=True)) #要有返回值\n",
    "                        model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    model.add(CuDNNLSTM(the_units))\n",
    "                    model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    for i in range(the_dense_layers):\n",
    "                        model.add(Dense(the_units,activation='relu'))  #全连接层\n",
    "                        model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    model.add(Dense(1)) #输出层\n",
    "\n",
    "                    model.compile(optimizer='adam' ,#优化器\n",
    "                                  loss = 'mse' ,#损失层\n",
    "                                  metrics=['mape'])#评价函数) #编译\n",
    "\n",
    "                    print(f'thread{thread_count},{code},NO.{build_models_times}:{the_mem_days},{the_lstm_layers},{the_dense_layers},{the_units},{str(datetime.datetime.now())}')\n",
    "                    model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test,y_test),verbose=model_verbose,callbacks=callback)\n",
    "#                     print('-----------------build_models_times-----------------')\n",
    "                    build_models_times+=1\n",
    "    return build_models_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30423223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.959982Z",
     "iopub.status.busy": "2023-03-05T21:03:42.959569Z",
     "iopub.status.idle": "2023-03-05T21:03:42.976574Z",
     "shell.execute_reply": "2023-03-05T21:03:42.975403Z"
    },
    "papermill": {
     "duration": 0.025949,
     "end_time": "2023-03-05T21:03:42.978960",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.953011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lstm_model_fit(begin,files,thread_count,klt):\n",
    "    thread_count = thread_count\n",
    "    try:\n",
    "        \n",
    "        for index in range(begin,len(files)):\n",
    "            #超时打包\n",
    "            print(f'already:{time.time()-_time_start},limit:{_time_limit*60*60}')\n",
    "            if time.time()-_time_start >_time_limit*60*60 :\n",
    "                # 停止所有线程\n",
    "#                 print(f'already:{time.time()-_time_start},limit:{_time_limit*60*60}')\n",
    "#                 for t in threads:\n",
    "#                     t.stop()\n",
    "                print('time out')\n",
    "                return\n",
    "            \n",
    "            code = files[index]\n",
    "            file_path = f'{BASE_PATH}/{klt}/{code}.gzip'\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "#                 fileName = file_path\n",
    "                print(f'thread{thread_count},{index},{code}:lstm_model_fit')\n",
    "                    \n",
    "                data = lstm_cleanm_data(file_path,klt)\n",
    "              \n",
    "                \n",
    "                print(f'fit_model：thread{thread_count},{index},{code}:lstm_cleanm_data_{file_path}')\n",
    "                fit_model = build_models(data.copy(deep=True),code,_mem_days,_lstm_layers,_dense_layers,_units,True,True,f'{thread_count},{index}',klt)\n",
    "                \n",
    "                print(f'log_df：thread{thread_count},{index},{code}:fit_model_{fit_model}')\n",
    "                \n",
    "                log_df = pd.read_csv(f'{log_file_name}/{klt}/{code}.csv', lineterminator='\\n', header=0)\n",
    "#                 print('-----------------min_loss_row-----------------')\n",
    "                min_loss_row = log_df.sort_values(by='loss',ascending=True)[0:1].to_dict(orient='records')[0]\n",
    "                # min_loss_row = log_df.loc[0:1,:].to_dict()\n",
    "\n",
    "                # log_df = log_df.sort_values(by='loss',axis=0,ascending=True)\n",
    "#                 print('-----------------最优解-----------------')\n",
    "                # # min_loss_row = log_df.iloc[0,:]\n",
    "                loss = min_loss_row['loss']\n",
    "                mape = min_loss_row['mape']\n",
    "                val_loss = min_loss_row['val_loss']\n",
    "                val_mape = min_loss_row['val_mape']\n",
    "                # the_mem_days, the_lstm_layers, the_dense_layers, the_units\n",
    "                _mem_day = int(min_loss_row['the_mem_days'])\n",
    "                _lstm_layer = int(min_loss_row['the_lstm_layers'])\n",
    "                _dense_layer = int(min_loss_row['the_dense_layers'])\n",
    "                _unit = int(min_loss_row['the_units'])\n",
    "                save_model = fit_model\n",
    "\n",
    "#                 save_model = build_models(data.copy(deep=True),code,[_mem_day],[_lstm_layer],[_dense_layer],[_unit],True,True,f'{thread_count},{index}')\n",
    "#                 return\n",
    "#                 print(f'thread{thread_count},{index},{code}:save_model_{save_model}')\n",
    "                # 把不符合标准的模型从csv和文件列表中删除\n",
    "#                 print('-----------------更新文件-----------------')\n",
    "                save_model_csv = pd.read_csv(model_saved_log)\n",
    "                #code被解析为int，再文件保存时，加上字符c保证解析为code\n",
    "                min_loss = save_model_csv.loc[save_model_csv['code'] == 'c'+code].sort_values('loss',ascending=True)[0:1].to_dict(orient='records')[0]['loss']\n",
    "                rows = save_model_csv.loc[(save_model_csv['code'] == 'c'+code )& (save_model_csv['loss'] > min_loss)]\n",
    "                for row in rows.to_dict(orient='records'):\n",
    "                    \n",
    "                    filename = row['modelname']\n",
    "                    if os.path.exists(filename):\n",
    "                        os.remove(filename)\n",
    "\n",
    "                save_model_csv = save_model_csv.drop(rows.index)\n",
    "                save_model_csv.to_csv(model_saved_log, index=False)\n",
    "                print(f'thread{thread_count},{index},{code}:save_model_csv_{model_saved_log}')\n",
    "            else:\n",
    "                print(f'code:{code};not exit')\n",
    "        print('-----------------完成循环-----------------')\n",
    "\n",
    "    except Exception as reason:\n",
    "        print(f'-----------------Exception-----------------')\n",
    "        if reason != '超时':\n",
    "            print(f'Exception:thread{thread_count},{index}:{str(reason)}')\n",
    "            exception_file = open(exception_file_full_name, 'a')\n",
    "\n",
    "            # 写表头code,loss,mape,val_loss,val_mape,modelname\n",
    "            exception_log = f'\\'{code}\\':{reason}\\n'\n",
    "            exception_file.write(model_log)\n",
    "            exception_file.close()\n",
    "\n",
    "            lstm_model_fit(index+1,files,thread_count,klt)\n",
    "        else:\n",
    "            print(str(reason))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17a5aa1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:03:42.990590Z",
     "iopub.status.busy": "2023-03-05T21:03:42.989542Z",
     "iopub.status.idle": "2023-03-05T21:08:03.052049Z",
     "shell.execute_reply": "2023-03-05T21:08:03.050422Z"
    },
    "papermill": {
     "duration": 260.071575,
     "end_time": "2023-03-05T21:08:03.055366",
     "exception": false,
     "start_time": "2023-03-05T21:03:42.983791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件范围0-9999\n",
      "already:0.0005826950073242188,limit:180.0\n",
      "already:0.0015227794647216797,limit:180.0\n",
      "already:0.002075672149658203,limit:180.0\n",
      "thread3,0,000797:lstm_model_fit\n",
      "thread2,0,000797:lstm_model_fit\n",
      "thread1,0,000797:lstm_model_fit\n",
      "fit_model：thread2,0,000797:lstm_cleanm_data_/kaggle/input/data03/data03/102/000797.gzip\n",
      "fit_model：thread3,0,000797:lstm_cleanm_data_/kaggle/input/data03/data03/103/000797.gzip\n",
      "fit_model：thread1,0,000797:lstm_cleanm_data_/kaggle/input/data03/data03/101/000797.gzip\n",
      "thread1,0,000797,NO.0:3,1,1,64,2023-03-05 21:03:50.219567\n",
      "thread2,0,000797,NO.0:3,1,1,64,2023-03-05 21:03:50.249202\n",
      "thread3,0,000797,NO.0:3,1,1,64,2023-03-05 21:03:50.261600\n",
      "log_df：thread3,0,000797:fit_model_1\n",
      "thread3,0,000797:save_model_csv_/kaggle/working/models_2/20230305Mar031678050222_models.csv\n",
      "already:72.96947598457336,limit:180.0\n",
      "thread3,1,002652:lstm_model_fit\n",
      "fit_model：thread3,1,002652:lstm_cleanm_data_/kaggle/input/data03/data03/103/002652.gzip\n",
      "thread3,1,002652,NO.0:3,1,1,64,2023-03-05 21:05:24.991104\n",
      "log_df：thread2,0,000797:fit_model_1\n",
      "thread2,0,000797:save_model_csv_/kaggle/working/models_2/20230305Mar031678050222_models.csv\n",
      "already:106.25034308433533,limit:180.0\n",
      "thread2,1,002652:lstm_model_fit\n",
      "log_df：thread3,1,002652:fit_model_1\n",
      "thread3,1,002652:save_model_csv_/kaggle/working/models_2/20230305Mar031678050222_models.csv\n",
      "already:115.32425570487976,limit:180.0\n",
      "thread3,2,000421:lstm_model_fit\n",
      "fit_model：thread3,2,000421:lstm_cleanm_data_/kaggle/input/data03/data03/103/000421.gzip\n",
      "thread3,2,000421,NO.0:3,1,1,64,2023-03-05 21:05:39.028312\n",
      "log_df：thread3,2,000421:fit_model_1\n",
      "thread3,2,000421:save_model_csv_/kaggle/working/models_2/20230305Mar031678050222_models.csv\n",
      "already:153.25772309303284,limit:180.0\n",
      "thread3,3,000800:lstm_model_fit\n",
      "fit_model：thread3,3,000800:lstm_cleanm_data_/kaggle/input/data03/data03/103/000800.gzip\n",
      "thread3,3,000800,NO.0:3,1,1,64,2023-03-05 21:06:16.799982\n",
      "log_df：thread3,3,000800:fit_model_1\n",
      "thread3,3,000800:save_model_csv_/kaggle/working/models_2/20230305Mar031678050222_models.csv\n",
      "already:185.00323057174683,limit:180.0\n",
      "time out\n",
      "log_df：thread1,0,000797:fit_model_1\n",
      "thread1,0,000797:save_model_csv_/kaggle/working/models_2/20230305Mar031678050222_models.csv\n",
      "already:190.388090133667,limit:180.0\n",
      "time out\n",
      "fit_model：thread2,1,002652:lstm_cleanm_data_/kaggle/input/data03/data03/102/002652.gzip\n",
      "thread2,1,002652,NO.0:3,1,1,64,2023-03-05 21:07:43.444324\n",
      "log_df：thread2,1,002652:fit_model_1\n",
      "thread2,1,002652:save_model_csv_/kaggle/working/models_2/20230305Mar031678050222_models.csv\n",
      "already:260.04856038093567,limit:180.0\n",
      "time out\n",
      "完成时间2023-03-05 21:08:03.047171,用时：260.0495948791504 s\n"
     ]
    }
   ],
   "source": [
    "# _data =lstm_cleanm_data( '/kaggle/input/stocks-data-20221216/301089.gzip')\n",
    "# ttt = build_models(_data.copy(deep=True),'301089',_mem_days,_lstm_layers,_dense_layers,_units,True,True,0)\n",
    "# #测试\n",
    "_mem_days=[3] #滑动区间，根据几天的数据做预测\n",
    "_lstm_layers,_dense_layers=[1],[1] #图层数\n",
    "_units= [64]\n",
    "\n",
    "batch_size=10 #每次训练在训练集中取batchsize个样本训练；.batch_size=1时为在线学习，也是标准的SGD,如果数据集比较小，则完全可以采用全数据集的形式;GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优\n",
    "epochs=100 #一个 epoch（代）是指整个数据集正向反向训练一次。\n",
    "\n",
    "\n",
    "model_verbose = 0\n",
    "\n",
    "_time_limit = 0.05\n",
    "# print(str(_lstm_layers))\n",
    "_time_start = time.time()\n",
    "\n",
    "_file_begin = 0\n",
    "_file_end = 9999\n",
    "#stocks_all,bankuai\n",
    "# files = os.listdir(path)\n",
    "print(f'文件范围{_file_begin}-{_file_end}')\n",
    "\n",
    "# lstm_model_fit(0,stocks_all[_file_begin:_file_end],0,101)\n",
    "\n",
    "stock_list = stocks_all+bankuai\n",
    "\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "t1 = threading.Thread(target=lstm_model_fit, args=(0,stock_list[_file_begin:_file_end],1,101))\n",
    "t1.start()\n",
    "threads.append(t1)\n",
    "\n",
    "t2 = threading.Thread(target=lstm_model_fit, args=(0,stock_list[_file_begin:_file_end],2,102))\n",
    "t2.start()\n",
    "threads.append(t2)\n",
    "\n",
    "t3 = threading.Thread(target=lstm_model_fit, args=(0,stock_list[_file_begin:_file_end],3,103))\n",
    "t3.start()\n",
    "threads.append(t3)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "comp_time = datetime.datetime.now()\n",
    "_time_end = time.time()\n",
    "\n",
    "print(f'完成时间{comp_time},用时：{_time_end-_time_start} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac0761fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:08:03.068270Z",
     "iopub.status.busy": "2023-03-05T21:08:03.067944Z",
     "iopub.status.idle": "2023-03-05T21:08:03.486754Z",
     "shell.execute_reply": "2023-03-05T21:08:03.484561Z"
    },
    "papermill": {
     "duration": 0.428396,
     "end_time": "2023-03-05T21:08:03.490073",
     "exception": false,
     "start_time": "2023-03-05T21:08:03.061677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打包完成2023-03-05 21:08:03.481834\n"
     ]
    }
   ],
   "source": [
    "file2zip('/kaggle/working/', '/kaggle/working/output.zip')\n",
    "print(f'打包完成{datetime.datetime.utcnow()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fabca3fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:08:03.502797Z",
     "iopub.status.busy": "2023-03-05T21:08:03.502429Z",
     "iopub.status.idle": "2023-03-05T21:08:03.513631Z",
     "shell.execute_reply": "2023-03-05T21:08:03.512625Z"
    },
    "papermill": {
     "duration": 0.020294,
     "end_time": "2023-03-05T21:08:03.516051",
     "exception": false,
     "start_time": "2023-03-05T21:08:03.495757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='output.zip' target='_blank'>output.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/output.zip"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink('output.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 295.572674,
   "end_time": "2023-03-05T21:08:06.444023",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-05T21:03:10.871349",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
