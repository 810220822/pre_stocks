{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a926947",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:34.584381Z",
     "iopub.status.busy": "2023-03-01T18:17:34.583217Z",
     "iopub.status.idle": "2023-03-01T18:17:36.603493Z",
     "shell.execute_reply": "2023-03-01T18:17:36.602149Z"
    },
    "papermill": {
     "duration": 2.032248,
     "end_time": "2023-03-01T18:17:36.606182",
     "exception": false,
     "start_time": "2023-03-01T18:17:34.573934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm models -r\n",
    "# !rm models_2 -r\n",
    "!mkdir models\n",
    "!mkdir models_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7568940f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:36.616999Z",
     "iopub.status.busy": "2023-03-01T18:17:36.616288Z",
     "iopub.status.idle": "2023-03-01T18:17:46.303138Z",
     "shell.execute_reply": "2023-03-01T18:17:46.301942Z"
    },
    "papermill": {
     "duration": 9.695263,
     "end_time": "2023-03-01T18:17:46.306054",
     "exception": false,
     "start_time": "2023-03-01T18:17:36.610791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler # pip3 install --upgrade --force-reinstall scikit-learn --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential #pip3 install --upgrade --force-reinstall keras --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
    "from tensorflow.keras.models import load_model #pip3 install --upgrade --force-reinstall keras --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
    "from tensorflow.keras.layers import LSTM,Dense,Dropout\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,Callback,CSVLogger,ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.utils import multi_gpu_utils\n",
    "import os\n",
    "from io import StringIO\n",
    "import gzip\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from shutil import copyfile\n",
    "# copy our file into the working directory (make sure it has .py suffix)\n",
    "copyfile(src = \"/kaggle/input/stocks-code/stocks.py\", dst = \"../working/stocks.py\")\n",
    " \n",
    "# import all our functions\n",
    "from stocks import stocks_all\n",
    "from stocks import bankuai\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #消除tensorflow警告\n",
    "\n",
    "model_saved_log_char = datetime.datetime.now().strftime('%Y%m%d%h%m%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d3034e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.317644Z",
     "iopub.status.busy": "2023-03-01T18:17:46.316935Z",
     "iopub.status.idle": "2023-03-01T18:17:46.324862Z",
     "shell.execute_reply": "2023-03-01T18:17:46.323720Z"
    },
    "papermill": {
     "duration": 0.016528,
     "end_time": "2023-03-01T18:17:46.327583",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.311055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#获取数据\n",
    "start = datetime.datetime(2000,1,1)\n",
    "end =  datetime.date.today()\n",
    "\n",
    "#参数整理\n",
    "EarlyStopping_monitor='val_loss' #monitor——被监测的量\n",
    "EarlyStopping_patience=10 #检测值停止变化的次数\n",
    "\n",
    "_mem_days=[1,3,5] #滑动区间，根据几天的数据做预测\n",
    "_lstm_layers,_dense_layers=[1,5],[1,5] #图层数\n",
    "# 这里我们设置的units=32的大小，其实代表得是LSTM单元内的隐藏层的尺寸。\n",
    "# 对于LSTM而言，每个单元有3个门，对应了4个激活函数（3个sigmoid,一个tanh）。也就是说有4个神经元数量为32的前馈网络层。\n",
    "_units= [32,64]\n",
    "\n",
    "# #测试\n",
    "# _mem_days=[3] #滑动区间，根据几天的数据做预测\n",
    "# _lstm_layers,_dense_layers=[1],[1] #图层数\n",
    "# _units= [32]\n",
    "\n",
    "\n",
    "optimizer='adam' #优化器:控制梯度下降和梯度爆炸\n",
    "loss = 'mse' #损失层\n",
    "metrics=['mape'] #评价函数\n",
    "batch_size=32 #每次训练在训练集中取batchsize个样本训练；.batch_size=1时为在线学习，也是标准的SGD,如果数据集比较小，则完全可以采用全数据集的形式;GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优\n",
    "epochs=50 #一个 epoch（代）是指整个数据集正向反向训练一次。\n",
    "\n",
    "model_verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f908c96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.337618Z",
     "iopub.status.busy": "2023-03-01T18:17:46.337305Z",
     "iopub.status.idle": "2023-03-01T18:17:46.343919Z",
     "shell.execute_reply": "2023-03-01T18:17:46.343014Z"
    },
    "papermill": {
     "duration": 0.014288,
     "end_time": "2023-03-01T18:17:46.346206",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.331918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#文件路径 data_\n",
    "# path = '/kaggle/input/stocks-data-20221216/'\n",
    "log_file_name = '/kaggle/working/models'\n",
    "model_saved_file='/kaggle/working/models_2'\n",
    "BASE_PATH = '/kaggle/input/all-stocks/all_stocks'\n",
    "\n",
    "model_saved_log = f'/kaggle/working/models_2/{ model_saved_log_char}_models.csv'\n",
    "\n",
    "# #创建任务总模型目录\n",
    "log_csv_file = open(model_saved_log, 'a')\n",
    "\n",
    "# 写表头code,loss,mape,val_loss,val_mape,modelname\n",
    "model_log = f'code,loss,mape,val_loss,val_mape,modelname\\n'\n",
    "log_csv_file.write(model_log)\n",
    "log_csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afc4ccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.356497Z",
     "iopub.status.busy": "2023-03-01T18:17:46.356205Z",
     "iopub.status.idle": "2023-03-01T18:17:46.362045Z",
     "shell.execute_reply": "2023-03-01T18:17:46.361045Z"
    },
    "papermill": {
     "duration": 0.013617,
     "end_time": "2023-03-01T18:17:46.364351",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.350734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exception_file_full_name = f'/kaggle/working/models_2/{ model_saved_log_char}_exception.txt'\n",
    "\n",
    "#创建异常文件\n",
    "exception_file = open(exception_file_full_name, 'a')\n",
    "\n",
    "# 写表头code,loss,mape,val_loss,val_mape,modelname\n",
    "exception_log = f'---------------Exception:{str(end)}------------------\\n'\n",
    "exception_file.write(exception_log)\n",
    "exception_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3a6e2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.375353Z",
     "iopub.status.busy": "2023-03-01T18:17:46.375032Z",
     "iopub.status.idle": "2023-03-01T18:17:46.394367Z",
     "shell.execute_reply": "2023-03-01T18:17:46.393042Z"
    },
    "papermill": {
     "duration": 0.027733,
     "end_time": "2023-03-01T18:17:46.396649",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.368916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#模型callback类\n",
    "class CustomCallback(Callback):\n",
    "#     print('-----------------CustomCallback-----------------')\n",
    "    code = ''\n",
    "    the_mem_days=0\n",
    "    the_lstm_layers=0\n",
    "    the_dense_layers=0\n",
    "    the_units = 0\n",
    "    csv_file_name = ''\n",
    "    model_path = ''\n",
    "    saveModelFile = False\n",
    "    saveModelLog = True\n",
    "\n",
    "    #epoch,loss,mape,val_loss,val_mape,code,the_mem_days,the_lstm_layers,the_dense_layers,the_units\n",
    "    csv_file = DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,path,csv_file_name,code,the_mem_days,the_lstm_layers,the_dense_layers,the_units,\n",
    "                 saveModelFile=False,saveModelLog=True):\n",
    "        self.model_path = path\n",
    "        self.csv_file_name = csv_file_name\n",
    "        self.code = code\n",
    "        self.the_mem_days = the_mem_days\n",
    "        self.the_lstm_layers = the_lstm_layers\n",
    "        self.the_dense_layers = the_dense_layers\n",
    "        self.the_units = the_units\n",
    "        self.saveModelFile = saveModelFile\n",
    "        self.saveModelLog=saveModelLog\n",
    "#         print('-----------------CustomCallback__init__-----------------')\n",
    "        #\n",
    "        if not os.path.exists(csv_file_name):\n",
    "#             print('-----------------os.path.exists(csv_file_name)-----------------')\n",
    "            # #创建任务总模型目录\n",
    "            _temp_file = open(csv_file_name, 'a') \n",
    "            _temp_file_header = f'epoch,loss,mape,val_loss,val_mape,code,the_mem_days,the_lstm_layers,the_dense_layers,the_units\\n'\n",
    "            _temp_file.write(_temp_file_header)\n",
    "            _temp_file.close()\n",
    "#         print('-----------------self.csv_file-----------------')\n",
    "        self.csv_file = pd.read_csv(csv_file_name, lineterminator='\\n', header=0)  \n",
    "                \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "#         print('-----------------self.on_epoch_end-----------------')\n",
    "        if self.saveModelFile == True:\n",
    "#             print('-----------------self.saveModelFile-----------------')\n",
    "            loss = logs['loss']\n",
    "            filepath =  f'{self.model_path}/{loss:.2f}_{self.code}_{epoch:02}_mem_{self.the_mem_days}_ltsm_{self.the_lstm_layers}_dense_{self.the_dense_layers}_unit_{self.the_units}.h5'\n",
    "\n",
    "            loss = logs['loss']\n",
    "            mape = logs['mape']\n",
    "            val_loss = logs['val_loss']\n",
    "            val_mape = logs['val_mape']\n",
    "            log_csv_file = open(model_saved_log, 'a+')\n",
    "            # code,loss,mape,val_loss,val_mape,modelname\n",
    "            model_log = f'c{self.code},{loss:.2f},{mape:.2f},{val_loss:.2f},{val_mape:.2f},{filepath}\\n'\n",
    "            log_csv_file.write(model_log)\n",
    "            log_csv_file.close()\n",
    "\n",
    "\n",
    "            self.model.save(filepath,save_format='h5')\n",
    "\n",
    "        if self.saveModelLog == True:\n",
    "#             print('-----------------self.saveModelFile-----------------')\n",
    "            if not math.isnan(logs['loss']) :\n",
    "#                 print('-----------------self.isnan-----------------')\n",
    "                _i_ = len(self.csv_file)\n",
    "                row = {\n",
    "                    'epoch':epoch,\n",
    "                    'loss' : float(round(logs['loss'],2) ),\n",
    "                    'mape':round(logs['mape'],2)  ,\n",
    "                    'val_loss': round(logs['val_loss'],2) ,\n",
    "                    'val_mape': round(logs['val_mape'],2)  ,\n",
    "\n",
    "                    'code': self.code,\n",
    "                    'the_mem_days': self.the_mem_days,\n",
    "                    'the_lstm_layers': self.the_lstm_layers,\n",
    "                    'the_dense_layers': self.the_dense_layers,\n",
    "                    'the_units': self.the_units\n",
    "                }\n",
    "\n",
    "                row_index = len(self.csv_file)\n",
    "                self.csv_file.loc[row_index] = row\n",
    "\n",
    "                self.csv_file.to_csv(self.csv_file_name,index=False)\n",
    "       \n",
    "                \n",
    "    def test(self):\n",
    "        print('suc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5817b807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.407613Z",
     "iopub.status.busy": "2023-03-01T18:17:46.406620Z",
     "iopub.status.idle": "2023-03-01T18:17:46.413811Z",
     "shell.execute_reply": "2023-03-01T18:17:46.412742Z"
    },
    "papermill": {
     "duration": 0.014575,
     "end_time": "2023-03-01T18:17:46.415896",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.401321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_dataframe(path):\n",
    "    os_file = open(path, 'rb')  # 打开压缩文件对象\n",
    "    file_stream = os_file.read()\n",
    "    message = gzip.decompress(file_stream).decode('GBK')\n",
    "\n",
    "    dataframe = pd.read_csv(StringIO(message), lineterminator='\\n', \n",
    "                            header=0,dtype={\"CODE\": str},index_col=\"日期\")\n",
    "    os_file.close()\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b606d030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.425538Z",
     "iopub.status.busy": "2023-03-01T18:17:46.425266Z",
     "iopub.status.idle": "2023-03-01T18:17:46.433318Z",
     "shell.execute_reply": "2023-03-01T18:17:46.432298Z"
    },
    "papermill": {
     "duration": 0.014979,
     "end_time": "2023-03-01T18:17:46.435233",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.420254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_nane_zero(x):\n",
    "    if x == 'None':\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "#打开数据，数据清洗1\n",
    "def lstm_cleanm_data(file_path,klt):\n",
    "    \n",
    "    data = open_dataframe(file_path)\n",
    "    \n",
    "    data = data.drop(['CODE'], axis=1)\n",
    "#     data = data.drop(['NAME'], axis=1)\n",
    "    data.sort_index(inplace=True,ascending=True)  # 排序\n",
    "    \n",
    "    #判断处理类型\n",
    "    if klt==101:\n",
    "        #(['CODE', '开盘', '收盘', '最高', '最低', '成交量', '成交额', '振幅', '涨跌额', '换手率'])\n",
    "        data = data[data.columns[0:9]]\n",
    "    \n",
    "    # 数据处理\n",
    "    for col in data.columns:\n",
    "        data[col].fillna(0, inplace=True)  # 将数学成绩为空值用0填\n",
    "        data[col] = data[col].map(set_nane_zero)\n",
    "    # 删除close的空值\n",
    "    for col in ['开盘', '收盘', '最高', '最低', '成交量', '成交额', '振幅', '涨跌额', '换手率']:\n",
    "        null_idx = data.loc[(data[col] == 0) | (data[col] == np.nan )|(data[col] == 'None')].index\n",
    "        data = data.drop(null_idx)\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9b1fbf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.445901Z",
     "iopub.status.busy": "2023-03-01T18:17:46.444960Z",
     "iopub.status.idle": "2023-03-01T18:17:46.454604Z",
     "shell.execute_reply": "2023-03-01T18:17:46.453595Z"
    },
    "papermill": {
     "duration": 0.016905,
     "end_time": "2023-03-01T18:17:46.456652",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.439747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#数据处理\n",
    "def stock_price_ltsm_data_processing(f,predays,isPredict:bool = False):\n",
    "    # 1.数据处理\n",
    "    predays = int(predays)\n",
    "    f['labels'] = f['收盘'].shift(-predays) #新增滑动列:f['labels'][predays]行等于f['labels'][0],predays正数向上滑动，负数向下滑动\n",
    "\n",
    "    x_data = f[:].iloc[:, :-1].values  # .values将dataframe转换为array\n",
    "\n",
    "    # 1.数据处理-归一化\n",
    "    scaler = StandardScaler() #数据标准化\n",
    "    sca_x = scaler.fit_transform(x_data) #除最后一例的所有行 把新增的滑动列去掉，再进行归一化\n",
    "\n",
    "    # date_begin = str(x_data[predays,:].index)\n",
    "\n",
    "    # 测试\n",
    "    # x_data = f[-(predays + n):].iloc[:, :-1]\n",
    "    #\n",
    "    # x = []  # 参数x\n",
    "    # y = []  # 输出参数y，用于模型巡礼和结果对比\n",
    "    # for i in range(predays, len(x_data)+1):\n",
    "    #     # 序列长度为19\n",
    "    #     x.append(x_data.iloc[i - predays:i, :])\n",
    "    #     # 标签长度为1\n",
    "    #     if i < len(x_data):\n",
    "    #         y.append(x_data.iloc[i, 0])  # 第i行第0个值，'tclose'\n",
    "    #     else:\n",
    "    #         y.append(np.nan)  # 预测值占位\n",
    "\n",
    "    #测试完\n",
    "    length = len(sca_x)\n",
    "    if isPredict == True:\n",
    "        length += 1\n",
    "\n",
    "    # 1.数据处理-分组，准备对照数据\n",
    "    x = []  # 参数x\n",
    "    y = []  # 输出参数y，用于模型巡礼和结果对比\n",
    "    for i in range(predays,length):\n",
    "        # 序列长度为19\n",
    "        x.append(sca_x[i - predays:i, :])\n",
    "        # 标签长度为1\n",
    "        if i < len(sca_x):\n",
    "            y.append(x_data[i, 0])  # 第i行第0个值，'tclose'\n",
    "        else:\n",
    "            y.append(np.nan)  # 预测值占位\n",
    "\n",
    "    # x_lately = x[predays:] #记录失效数据\n",
    "    # x = x[:predays] #删除失效数据\n",
    "\n",
    "    x,y= np.array(x), np.array(y)\n",
    "    x = np.reshape(x,(x.shape[0],x.shape[1],x.shape[2]))\n",
    "\n",
    "\n",
    "\n",
    "    return  x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d356cba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.466361Z",
     "iopub.status.busy": "2023-03-01T18:17:46.465838Z",
     "iopub.status.idle": "2023-03-01T18:17:46.470008Z",
     "shell.execute_reply": "2023-03-01T18:17:46.469018Z"
    },
    "papermill": {
     "duration": 0.011268,
     "end_time": "2023-03-01T18:17:46.472305",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.461037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_time_start = time.time()\n",
    "_time_limit = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "122ec29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.483204Z",
     "iopub.status.busy": "2023-03-01T18:17:46.481696Z",
     "iopub.status.idle": "2023-03-01T18:17:46.488949Z",
     "shell.execute_reply": "2023-03-01T18:17:46.488020Z"
    },
    "papermill": {
     "duration": 0.014853,
     "end_time": "2023-03-01T18:17:46.491310",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.476457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    " \n",
    "\n",
    "def file2zip(packagePath, zipPath):\n",
    "    '''\n",
    "  :param packagePath: 文件夹路径\n",
    "  :param zipPath: 压缩包路径\n",
    "  :return:\n",
    "  '''\n",
    "    if os.path.exists(zipPath):\n",
    "        os.remove(zipPath)\n",
    "    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n",
    "    for path, dirNames, fileNames in os.walk(packagePath):\n",
    "        fpath = path.replace(packagePath, '')\n",
    "        for name in fileNames:\n",
    "            fullName = os.path.join(path, name)\n",
    "            name = fpath + '\\\\' + name\n",
    "            zip.write(fullName, name)\n",
    "    zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "391257e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.501229Z",
     "iopub.status.busy": "2023-03-01T18:17:46.500908Z",
     "iopub.status.idle": "2023-03-01T18:17:46.514396Z",
     "shell.execute_reply": "2023-03-01T18:17:46.513479Z"
    },
    "papermill": {
     "duration": 0.021219,
     "end_time": "2023-03-01T18:17:46.516640",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.495421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#建模\n",
    "def build_models(f,code,mem_days,lstm_layers,dense_layers,units,saveModelFile ,saveModelLog,thread_count ):\n",
    "    \n",
    "    build_models_times = 0\n",
    "\n",
    "    for the_mem_days in mem_days:\n",
    "        new_df = f.copy(deep=True)\n",
    "        x, y = stock_price_ltsm_data_processing(new_df,the_mem_days,False)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, shuffle=False, test_size=0.2)\n",
    "        for the_lstm_layers in lstm_layers:\n",
    "            for the_dense_layers in dense_layers:\n",
    "                for the_units in units:\n",
    "#                     print('-----------------callback-----------------')\n",
    "                    callback = [\n",
    "                        EarlyStopping(monitor=EarlyStopping_monitor, patience=EarlyStopping_patience),\n",
    "                        # CSVLogger(filename, separator=',', append=True),\n",
    "                        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto',\n",
    "                                          min_delta=0.0001, cooldown=0, min_lr=0),\n",
    "                        CustomCallback(model_saved_file,f'{log_file_name}/{code}.csv',code,the_mem_days,the_lstm_layers,the_dense_layers,the_units,\n",
    "                                       saveModelFile=saveModelFile,saveModelLog=saveModelLog)\n",
    "                    ]\n",
    "#                     print('-----------------Sequential-----------------')\n",
    "                    #构建神经网络\n",
    "                    model = Sequential()\n",
    "#                     from keras.utils import multi_gpu_utils\n",
    "#                     model = multi_gpu_utils(model, gpus=2)\n",
    "                    model.add(CuDNNLSTM(the_units,input_shape=x.shape[1:],return_sequences=True)) #第一层\n",
    "                    model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    for i in range(the_lstm_layers):\n",
    "                        model.add(CuDNNLSTM(the_units,return_sequences=True)) #要有返回值\n",
    "                        model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    model.add(CuDNNLSTM(the_units))\n",
    "                    model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    for i in range(the_dense_layers):\n",
    "                        model.add(Dense(the_units,activation='relu'))  #全连接层\n",
    "                        model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    model.add(Dense(1)) #输出层\n",
    "\n",
    "                    model.compile(optimizer='adam' ,#优化器\n",
    "                                  loss = 'mse' ,#损失层\n",
    "                                  metrics=['mape'])#评价函数) #编译\n",
    "\n",
    "                    print(f'thread{thread_count},{code},NO.{build_models_times}:{the_mem_days},{the_lstm_layers},{the_dense_layers},{the_units},{str(datetime.datetime.now())}')\n",
    "                    model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test,y_test),verbose=model_verbose,callbacks=callback)\n",
    "#                     print('-----------------build_models_times-----------------')\n",
    "                    build_models_times+=1\n",
    "    return build_models_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee103a3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.526824Z",
     "iopub.status.busy": "2023-03-01T18:17:46.526557Z",
     "iopub.status.idle": "2023-03-01T18:17:46.542042Z",
     "shell.execute_reply": "2023-03-01T18:17:46.540907Z"
    },
    "papermill": {
     "duration": 0.023102,
     "end_time": "2023-03-01T18:17:46.544257",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.521155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lstm_model_fit(begin,files,thread_count,klt):\n",
    "    thread_count = thread_count\n",
    "    try:\n",
    "        \n",
    "        for index in range(begin,len(files)):\n",
    "            #超时打包\n",
    "            if time.time()-_time_start >_time_limit*60*60 :\n",
    "                print('time out')\n",
    "                return\n",
    "            \n",
    "            code = files[index]\n",
    "            file_path = f'{BASE_PATH}/{klt}/{code}.gzip'\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "#                 fileName = file_path\n",
    "                print(f'thread{thread_count},{index},{code}:lstm_model_fit')\n",
    "                    \n",
    "                data = lstm_cleanm_data(file_path,klt)\n",
    "              \n",
    "                \n",
    "                print(f'fit_model：thread{thread_count},{index},{code}:lstm_cleanm_data_{file_path}')\n",
    "                fit_model = build_models(data.copy(deep=True),code,_mem_days,_lstm_layers,_dense_layers,_units,True,True,f'{thread_count},{index}')\n",
    "                \n",
    "                print(f'log_df：thread{thread_count},{index},{code}:fit_model_{fit_model}')\n",
    "                \n",
    "                log_df = pd.read_csv(f'{log_file_name}/{code}.csv', lineterminator='\\n', header=0)\n",
    "#                 print('-----------------min_loss_row-----------------')\n",
    "                min_loss_row = log_df.sort_values(by='loss',ascending=True)[0:1].to_dict(orient='records')[0]\n",
    "                # min_loss_row = log_df.loc[0:1,:].to_dict()\n",
    "\n",
    "                # log_df = log_df.sort_values(by='loss',axis=0,ascending=True)\n",
    "#                 print('-----------------最优解-----------------')\n",
    "                # # min_loss_row = log_df.iloc[0,:]\n",
    "                loss = min_loss_row['loss']\n",
    "                mape = min_loss_row['mape']\n",
    "                val_loss = min_loss_row['val_loss']\n",
    "                val_mape = min_loss_row['val_mape']\n",
    "                # the_mem_days, the_lstm_layers, the_dense_layers, the_units\n",
    "                _mem_day = int(min_loss_row['the_mem_days'])\n",
    "                _lstm_layer = int(min_loss_row['the_lstm_layers'])\n",
    "                _dense_layer = int(min_loss_row['the_dense_layers'])\n",
    "                _unit = int(min_loss_row['the_units'])\n",
    "                save_model = fit_model\n",
    "\n",
    "#                 save_model = build_models(data.copy(deep=True),code,[_mem_day],[_lstm_layer],[_dense_layer],[_unit],True,True,f'{thread_count},{index}')\n",
    "#                 return\n",
    "#                 print(f'thread{thread_count},{index},{code}:save_model_{save_model}')\n",
    "                # 把不符合标准的模型从csv和文件列表中删除\n",
    "#                 print('-----------------更新文件-----------------')\n",
    "                save_model_csv = pd.read_csv(model_saved_log)\n",
    "                #code被解析为int，再文件保存时，加上字符c保证解析为code\n",
    "                min_loss = save_model_csv.loc[save_model_csv['code'] == 'c'+code].sort_values('loss',ascending=True)[0:1].to_dict(orient='records')[0]['loss']\n",
    "                rows = save_model_csv.loc[(save_model_csv['code'] == 'c'+code )& (save_model_csv['loss'] > min_loss)]\n",
    "                for row in rows.to_dict(orient='records'):\n",
    "                    \n",
    "                    filename = row['modelname']\n",
    "                    if os.path.exists(filename):\n",
    "                        os.remove(filename)\n",
    "\n",
    "                save_model_csv = save_model_csv.drop(rows.index)\n",
    "                save_model_csv.to_csv(model_saved_log, index=False)\n",
    "                print(f'thread{thread_count},{index},{code}:save_model_csv_{model_saved_log}')\n",
    "            else:\n",
    "                print(f'code:{code};not exit')\n",
    "        print('-----------------完成循环-----------------')\n",
    "\n",
    "    except Exception as reason:\n",
    "        print(f'-----------------Exception-----------------')\n",
    "        if reason != '超时':\n",
    "            print(f'Exception:thread{thread_count},{index}:{str(reason)}')\n",
    "            exception_file = open(exception_file_full_name, 'a')\n",
    "\n",
    "            # 写表头code,loss,mape,val_loss,val_mape,modelname\n",
    "            exception_log = f'\\'{code}\\':{reason}\\n'\n",
    "            exception_file.write(model_log)\n",
    "            exception_file.close()\n",
    "\n",
    "            lstm_model_fit(index+1,files,thread_count,klt)\n",
    "        else:\n",
    "            print(str(reason))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1380519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:17:46.554570Z",
     "iopub.status.busy": "2023-03-01T18:17:46.553690Z",
     "iopub.status.idle": "2023-03-01T18:19:20.248219Z",
     "shell.execute_reply": "2023-03-01T18:19:20.247063Z"
    },
    "papermill": {
     "duration": 93.708043,
     "end_time": "2023-03-01T18:19:20.256495",
     "exception": false,
     "start_time": "2023-03-01T18:17:46.548452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件范围1860-1861\n",
      "thread0,0,002326:lstm_model_fit\n",
      "fit_model：thread0,0,002326:lstm_cleanm_data_/kaggle/input/all-stocks/all_stocks/101/002326.gzip\n",
      "thread0,0,002326,NO.0:3,1,1,64,2023-03-01 18:17:51.036505\n",
      "log_df：thread0,0,002326:fit_model_1\n",
      "thread0,0,002326:save_model_csv_/kaggle/working/models_2/20230301Mar031677694666_models.csv\n",
      "-----------------完成循环-----------------\n",
      "完成时间2023-03-01 18:19:20.244566,用时：93.68566679954529 s\n"
     ]
    }
   ],
   "source": [
    "# _data =lstm_cleanm_data( '/kaggle/input/stocks-data-20221216/301089.gzip')\n",
    "# ttt = build_models(_data.copy(deep=True),'301089',_mem_days,_lstm_layers,_dense_layers,_units,True,True,0)\n",
    "# #测试\n",
    "_mem_days=[3] #滑动区间，根据几天的数据做预测\n",
    "_lstm_layers,_dense_layers=[1],[1] #图层数\n",
    "_units= [64]\n",
    "\n",
    "batch_size=10 #每次训练在训练集中取batchsize个样本训练；.batch_size=1时为在线学习，也是标准的SGD,如果数据集比较小，则完全可以采用全数据集的形式;GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优\n",
    "epochs=100 #一个 epoch（代）是指整个数据集正向反向训练一次。\n",
    "\n",
    "\n",
    "model_verbose = 0\n",
    "\n",
    "_time_limit = 10.2\n",
    "# print(str(_lstm_layers))\n",
    "_time_start = time.time()\n",
    "\n",
    "_file_begin = 1860\n",
    "_file_end = 1861\n",
    "#stocks_all,bankuai\n",
    "# files = os.listdir(path)\n",
    "print(f'文件范围{_file_begin}-{_file_end}')\n",
    "\n",
    "lstm_model_fit(0,stocks_all[_file_begin:_file_end],0,101)\n",
    "# threding_lstm(3,0,file[_file_begin:_file_end])\n",
    "\n",
    "comp_time = datetime.datetime.now()\n",
    "_time_end = time.time()\n",
    "print(f'完成时间{comp_time},用时：{_time_end-_time_start} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "808aa6ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:19:20.268598Z",
     "iopub.status.busy": "2023-03-01T18:19:20.268008Z",
     "iopub.status.idle": "2023-03-01T18:19:20.340667Z",
     "shell.execute_reply": "2023-03-01T18:19:20.339013Z"
    },
    "papermill": {
     "duration": 0.081107,
     "end_time": "2023-03-01T18:19:20.343558",
     "exception": false,
     "start_time": "2023-03-01T18:19:20.262451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打包完成2023-03-01 18:19:20.336732\n"
     ]
    }
   ],
   "source": [
    "file2zip('/kaggle/working/', '/kaggle/working/output.zip')\n",
    "print(f'打包完成{datetime.datetime.utcnow()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d991d22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T18:19:20.354527Z",
     "iopub.status.busy": "2023-03-01T18:19:20.354151Z",
     "iopub.status.idle": "2023-03-01T18:19:20.363210Z",
     "shell.execute_reply": "2023-03-01T18:19:20.362048Z"
    },
    "papermill": {
     "duration": 0.017454,
     "end_time": "2023-03-01T18:19:20.365675",
     "exception": false,
     "start_time": "2023-03-01T18:19:20.348221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='output.zip' target='_blank'>output.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/output.zip"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink('output.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 118.930671,
   "end_time": "2023-03-01T18:19:23.292105",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-01T18:17:24.361434",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
