{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77599df",
   "metadata": {
    "papermill": {
     "duration": 0.006126,
     "end_time": "2023-03-12T03:37:56.859295",
     "exception": false,
     "start_time": "2023-03-12T03:37:56.853169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 对完成处理的股票交易数据（分组、归一化）做lstm建模\n",
    "输入：处理好的数据\n",
    "输出：模型和模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d95fc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:37:56.871175Z",
     "iopub.status.busy": "2023-03-12T03:37:56.870714Z",
     "iopub.status.idle": "2023-03-12T03:37:56.876427Z",
     "shell.execute_reply": "2023-03-12T03:37:56.875321Z"
    },
    "papermill": {
     "duration": 0.015118,
     "end_time": "2023-03-12T03:37:56.879541",
     "exception": false,
     "start_time": "2023-03-12T03:37:56.864423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46190f36",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-12T03:37:56.891742Z",
     "iopub.status.busy": "2023-03-12T03:37:56.890939Z",
     "iopub.status.idle": "2023-03-12T03:38:07.519542Z",
     "shell.execute_reply": "2023-03-12T03:38:07.518153Z"
    },
    "papermill": {
     "duration": 10.638187,
     "end_time": "2023-03-12T03:38:07.522627",
     "exception": false,
     "start_time": "2023-03-12T03:37:56.884440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler # pip3 install --upgrade --force-reinstall scikit-learn --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential #pip3 install --upgrade --force-reinstall keras --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
    "from tensorflow.keras.models import load_model #pip3 install --upgrade --force-reinstall keras --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
    "from tensorflow.keras.layers import LSTM,Dense,Dropout\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,Callback,CSVLogger,ReduceLROnPlateau\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from keras.utils import multi_gpu_utils\n",
    "import os\n",
    "from io import StringIO\n",
    "import gzip\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "from shutil import copyfile\n",
    "# copy our file into the working directory (make sure it has .py suffix)\n",
    "copyfile(src = \"/kaggle/input/stocks-code/stocks.py\", dst = \"../working/stocks.py\")\n",
    " \n",
    "# import all our functions\n",
    "from stocks import stocks_all\n",
    "from stocks import bankuai\n",
    "\n",
    "import pickle\n",
    "\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "threads = []\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #消除tensorflow警告\n",
    "\n",
    "model_saved_log_char = datetime.datetime.now().strftime('%Y%m%d%h%m%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63dfc24a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.534999Z",
     "iopub.status.busy": "2023-03-12T03:38:07.534180Z",
     "iopub.status.idle": "2023-03-12T03:38:07.543238Z",
     "shell.execute_reply": "2023-03-12T03:38:07.541798Z"
    },
    "papermill": {
     "duration": 0.018408,
     "end_time": "2023-03-12T03:38:07.546054",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.527646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#获取数据\n",
    "start = datetime.datetime(2000,1,1)\n",
    "end =  datetime.date.today()\n",
    "\n",
    "#参数整理\n",
    "EarlyStopping_monitor='val_loss' #monitor——被监测的量\n",
    "EarlyStopping_patience=10 #检测值停止变化的次数\n",
    "\n",
    "_mem_days=[1,3,5] #滑动区间，根据几天的数据做预测\n",
    "_lstm_layers,_dense_layers=[1,5],[1,5] #图层数\n",
    "# 这里我们设置的units=32的大小，其实代表得是LSTM单元内的隐藏层的尺寸。\n",
    "# 对于LSTM而言，每个单元有3个门，对应了4个激活函数（3个sigmoid,一个tanh）。也就是说有4个神经元数量为32的前馈网络层。\n",
    "_units= [32,64]\n",
    "\n",
    "# #测试\n",
    "# _mem_days=[3] #滑动区间，根据几天的数据做预测\n",
    "# _lstm_layers,_dense_layers=[1],[1] #图层数\n",
    "# _units= [32]\n",
    "\n",
    "\n",
    "optimizer='adam' #优化器:控制梯度下降和梯度爆炸\n",
    "loss = 'mse' #损失层\n",
    "metrics=['mape'] #评价函数\n",
    "batch_size=32 #每次训练在训练集中取batchsize个样本训练；.batch_size=1时为在线学习，也是标准的SGD,如果数据集比较小，则完全可以采用全数据集的形式;GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优\n",
    "epochs=50 #一个 epoch（代）是指整个数据集正向反向训练一次。\n",
    "\n",
    "model_verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1701338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.557958Z",
     "iopub.status.busy": "2023-03-12T03:38:07.557552Z",
     "iopub.status.idle": "2023-03-12T03:38:07.567339Z",
     "shell.execute_reply": "2023-03-12T03:38:07.566099Z"
    },
    "papermill": {
     "duration": 0.019663,
     "end_time": "2023-03-12T03:38:07.570736",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.551073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#文件路径 data_\n",
    "# path = '/kaggle/input/stocks-data-20221216/'\n",
    "log_file_name = '/kaggle/working/models'\n",
    "model_saved_file='/kaggle/working/models_2'\n",
    "INPUT_PATH = '/kaggle/input/stock-prefit-0306/data_pre_fit'\n",
    "\n",
    "\n",
    "\n",
    "[os.makedirs(f\"{log_file_name}/{klt}\", exist_ok=True) for klt in [101, 102, 103]]\n",
    "[os.makedirs(f\"{model_saved_file}/{klt}\", exist_ok=True) for klt in [101, 102, 103]]\n",
    "\n",
    "for _klt_ in [101,102,103]:\n",
    "    model_saved_log = f'{model_saved_file}/{_klt_}/{ model_saved_log_char}_models.csv'\n",
    "    # #创建任务总模型目录\n",
    "    log_csv_file = open(model_saved_log, 'a')\n",
    "\n",
    "    # 写表头code,loss,mape,val_loss,val_mape,modelname\n",
    "    model_log = f'code,klt,loss,mape,val_loss,val_mape,modelname\\n'\n",
    "    log_csv_file.write(model_log)\n",
    "    log_csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085c7f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.582974Z",
     "iopub.status.busy": "2023-03-12T03:38:07.582566Z",
     "iopub.status.idle": "2023-03-12T03:38:07.589518Z",
     "shell.execute_reply": "2023-03-12T03:38:07.588074Z"
    },
    "papermill": {
     "duration": 0.016827,
     "end_time": "2023-03-12T03:38:07.592785",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.575958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exception_file_full_name = f'{model_saved_file}/{ model_saved_log_char}_exception.txt'\n",
    "\n",
    "#创建异常文件\n",
    "exception_file = open(exception_file_full_name, 'a')\n",
    "\n",
    "# 写表头code,loss,mape,val_loss,val_mape,modelname\n",
    "exception_log = f'---------------Exception:{str(end)}------------------\\n'\n",
    "exception_file.write(exception_log)\n",
    "exception_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1167c8f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.605367Z",
     "iopub.status.busy": "2023-03-12T03:38:07.604918Z",
     "iopub.status.idle": "2023-03-12T03:38:07.627010Z",
     "shell.execute_reply": "2023-03-12T03:38:07.625817Z"
    },
    "papermill": {
     "duration": 0.031819,
     "end_time": "2023-03-12T03:38:07.629883",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.598064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#模型callback类\n",
    "class CustomCallback(Callback):\n",
    "#     print('-----------------CustomCallback-----------------')\n",
    "    code = ''\n",
    "    the_mem_days=0\n",
    "    the_lstm_layers=0\n",
    "    the_dense_layers=0\n",
    "    the_units = 0\n",
    "    csv_file_name = ''\n",
    "    model_path = ''\n",
    "    saveModelFile = False\n",
    "    saveModelLog = True\n",
    "\n",
    "    #epoch,loss,mape,val_loss,val_mape,code,the_mem_days,the_lstm_layers,the_dense_layers,the_units\n",
    "    csv_file = DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,path,csv_file_name,code,the_mem_days,the_lstm_layers,the_dense_layers,the_units,\n",
    "                 saveModelFile=False,saveModelLog=True,klt=101):\n",
    "#         print(f'-----------------path:{path},klt:{klt},code:{code}-----------------\\n')\n",
    "        self.model_path = path\n",
    "        self.csv_file_name = csv_file_name\n",
    "        self.code = code\n",
    "        self.the_mem_days = the_mem_days\n",
    "        self.the_lstm_layers = the_lstm_layers\n",
    "        self.the_dense_layers = the_dense_layers\n",
    "        self.the_units = the_units\n",
    "        self.saveModelFile = saveModelFile\n",
    "        self.saveModelLog=saveModelLog\n",
    "        self.klt = klt\n",
    "#         print(f'-----------------CustomCallback__init__,klt:{klt},code:{code}-----------------\\n')\n",
    "        #\n",
    "        if not os.path.exists(csv_file_name):\n",
    "#             print(f'-----------------os.path.exists(csv_file_name),klt:{klt},code:{code}-----------------\\n')\n",
    "            # #创建任务总模型目录\n",
    "            _temp_file = open(csv_file_name, 'a') \n",
    "            _temp_file_header = f'epoch,loss,mape,val_loss,val_mape,code,klt,the_mem_days,the_lstm_layers,the_dense_layers,the_units\\n'\n",
    "#             print(f'-----------------_temp_file_header:{_temp_file_header},klt:{klt},code:{code}-----------------\\n')\n",
    "            _temp_file.write(_temp_file_header)\n",
    "            _temp_file.close()\n",
    "#         print(f'-----------------self.csv_file,klt:{klt},code:{code}-----------------\\n')\n",
    "        self.csv_file = pd.read_csv(csv_file_name, lineterminator='\\n', header=0)  \n",
    "                \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "#         print(f'-----------------self.on_epoch_end,klt:{self.klt},code:{self.code}-----------------\\n')\n",
    "        if self.saveModelFile == True:\n",
    "#             print(f'-----------------self.saveModelFile,klt:{self.klt},code:{self.code}-----------------\\n')\n",
    "            loss = logs['loss']\n",
    "            filepath =  f'{self.model_path}/{loss:.2f}_{self.code}_{epoch:02}_mem_{self.the_mem_days}_ltsm_{self.the_lstm_layers}_dense_{self.the_dense_layers}_unit_{self.the_units}.h5'\n",
    "#             print(f'-----------------filepath:{filepath}-----------------\\n')\n",
    "            loss = logs['loss']\n",
    "            mape = logs['mape']\n",
    "            val_loss = logs['val_loss']\n",
    "            val_mape = logs['val_mape']\n",
    "            model_saved_log1 = f'{model_saved_file}/{self.klt}/{ model_saved_log_char}_models.csv'\n",
    "#             print(f'-----------------model_saved_log:{model_saved_log1},klt:{self.klt},code:{self.code}-----------------\\n')\n",
    "            log_csv_file = open(model_saved_log1, 'a+')\n",
    "            # code,loss,mape,val_loss,val_mape,modelname\n",
    "            model_log = f'c{self.code},{self.klt},{loss:.2f},{mape:.2f},{val_loss:.2f},{val_mape:.2f},{filepath}\\n'\n",
    "            log_csv_file.write(model_log)\n",
    "            log_csv_file.close()\n",
    "            \n",
    "#             print(f'-----------------filepath:{filepath},klt:{self.klt},code:{self.code}-----------------\\n')\n",
    "\n",
    "            self.model.save(filepath,save_format='h5')\n",
    "#         print(f'-----------------self.saveModelLog:{self.saveModelLog},klt:{self.klt},code:{self.code}-----------------\\n')\n",
    "        if self.saveModelLog == True:\n",
    "#             print(f'-----------------logs.loss{logs['loss']}-----------------\\n')\n",
    "            if not math.isnan(logs['loss']) :\n",
    "#                 print(f'-----------------self.csv_file{self.csv_file},klt:{klt},code:{code}-----------------\\n')\n",
    "                _i_ = len(self.csv_file)\n",
    "#                 print(f'-----------------_i_:{_i_},klt:{self.klt},code:{self.code}-----------------\\n')\n",
    "                row = {\n",
    "                    'epoch':epoch,\n",
    "                    'loss' : float(round(logs['loss'],2) ),\n",
    "                    'mape':round(logs['mape'],2)  ,\n",
    "                    'val_loss': round(logs['val_loss'],2) ,\n",
    "                    'val_mape': round(logs['val_mape'],2)  ,\n",
    "\n",
    "                    'code': self.code,\n",
    "                    'klt':self.klt,\n",
    "                    'the_mem_days': self.the_mem_days,\n",
    "                    'the_lstm_layers': self.the_lstm_layers,\n",
    "                    'the_dense_layers': self.the_dense_layers,\n",
    "                    'the_units': self.the_units\n",
    "                }\n",
    "#                 print(f'-----------------row{row},klt:{self.klt},code:{self.code}-----------------\\n')\n",
    "                row_index = len(self.csv_file)\n",
    "#                 print(f'-----------------self.csv_file.loc:{313},klt:{self.klt},code:{self.code}-----------------\\n')\n",
    "                self.csv_file.loc[row_index] = row\n",
    "#                 print(f'-----------------{self.csv_file_name},klt:{self.klt},code:{self.code}-----------------\\n')\n",
    "                self.csv_file.to_csv(self.csv_file_name,index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bf6af00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.642435Z",
     "iopub.status.busy": "2023-03-12T03:38:07.641996Z",
     "iopub.status.idle": "2023-03-12T03:38:07.647381Z",
     "shell.execute_reply": "2023-03-12T03:38:07.646411Z"
    },
    "papermill": {
     "duration": 0.014453,
     "end_time": "2023-03-12T03:38:07.649770",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.635317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_time_start = time.time()\n",
    "_time_limit = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8a23e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.662289Z",
     "iopub.status.busy": "2023-03-12T03:38:07.661232Z",
     "iopub.status.idle": "2023-03-12T03:38:07.678295Z",
     "shell.execute_reply": "2023-03-12T03:38:07.676594Z"
    },
    "papermill": {
     "duration": 0.026479,
     "end_time": "2023-03-12T03:38:07.681208",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.654729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#建模\n",
    "def build_models(file_path,code,mem_days,lstm_layers,dense_layers,units,saveModelFile ,saveModelLog,thread_count,klt ):\n",
    "    \n",
    "    build_models_times = 0\n",
    "\n",
    "    for the_mem_days in mem_days:\n",
    "#         new_df = f\n",
    "        x, y = open_data_processd(file_path,klt)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, shuffle=False, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # 转换为 Dataset 对象\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "\n",
    "        for the_lstm_layers in lstm_layers:\n",
    "            for the_dense_layers in dense_layers:\n",
    "                for the_units in units:\n",
    "                    \n",
    "                    callback = [EarlyStopping(monitor=EarlyStopping_monitor, patience=EarlyStopping_patience),\n",
    "                        # CSVLogger(filename, separator=',', append=True),\n",
    "                        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto',\n",
    "                                          min_delta=0.0001, cooldown=0, min_lr=0),\n",
    "                        CustomCallback(f'{model_saved_file}/{klt}',f'{log_file_name}/{klt}/{code}.csv',code,the_mem_days,the_lstm_layers,the_dense_layers,the_units,\n",
    "                                       saveModelFile=saveModelFile,saveModelLog=saveModelLog,klt=klt)]\n",
    "                    \n",
    "                     #构建神经网络\n",
    "                    model = Sequential()\n",
    "                    model.add(LSTM(the_units,input_shape=x.shape[1:],return_sequences=True)) #第一层\n",
    "                    model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    for i in range(the_lstm_layers):\n",
    "                        model.add(LSTM(the_units,return_sequences=True)) #要有返回值\n",
    "                        model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    model.add(LSTM(the_units))\n",
    "                    model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    for i in range(the_dense_layers):\n",
    "                        model.add(Dense(the_units,activation='relu'))  #全连接层\n",
    "                        model.add(Dropout(0.1)) #防止过拟合\n",
    "\n",
    "                    model.add(Dense(1)) #输出层\n",
    "\n",
    "                    model.compile(optimizer='adam' ,#优化器\n",
    "                                  loss = 'mse' ,#损失层\n",
    "                                  metrics=['mape'])#评价函数) #编译\n",
    "\n",
    "                    print(f'thread{thread_count},{code},NO.{build_models_times}:{the_mem_days},{the_lstm_layers},{the_dense_layers},{the_units},{str(datetime.datetime.now())}')\n",
    "#                      \n",
    "                    model.fit(train_dataset.prefetch(tf.data.experimental.AUTOTUNE),epochs=epochs,validation_data=(x_test,y_test),verbose=model_verbose,callbacks=callback)\n",
    "#                     \n",
    "                    build_models_times+=1\n",
    "                    del  x_train, x_test, y_train, y_test,train_dataset,model\n",
    "                    \n",
    "    \n",
    "    return build_models_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8282a2f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.693071Z",
     "iopub.status.busy": "2023-03-12T03:38:07.692623Z",
     "iopub.status.idle": "2023-03-12T03:38:07.699253Z",
     "shell.execute_reply": "2023-03-12T03:38:07.697990Z"
    },
    "papermill": {
     "duration": 0.015665,
     "end_time": "2023-03-12T03:38:07.701847",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.686182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_data_processd(file_path,klt):\n",
    "    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        \n",
    "    return x, y\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d49459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.714519Z",
     "iopub.status.busy": "2023-03-12T03:38:07.714098Z",
     "iopub.status.idle": "2023-03-12T03:38:07.731206Z",
     "shell.execute_reply": "2023-03-12T03:38:07.730090Z"
    },
    "papermill": {
     "duration": 0.026993,
     "end_time": "2023-03-12T03:38:07.733889",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.706896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lstm_model_fit(begin,files,thread_count,klt):\n",
    "    thread_count = thread_count\n",
    "    try:\n",
    "        \n",
    "        for index in range(begin,len(files)):\n",
    "            #超时打包\n",
    "#             print(f'already:{time.time()-_time_start},limit:{_time_limit*60*60}')\n",
    "            if time.time()-_time_start >_time_limit*60*60 :\n",
    "                print('time out')\n",
    "                break\n",
    "            \n",
    "            if os.path.exists('/kaggle/working/stop'):\n",
    "                \n",
    "              #退出循环\n",
    "                break\n",
    "            \n",
    "            code = files[index]\n",
    "            file_path = f'{INPUT_PATH}/{klt}/{code}.pkl'\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "#                  \n",
    "                fit_model = build_models(file_path,code,_mem_days,_lstm_layers,_dense_layers,_units,True,True,f'{thread_count},{index}',klt)\n",
    "                 \n",
    "                log_df = pd.read_csv(f'{log_file_name}/{klt}/{code}.csv', lineterminator='\\n', header=0)\n",
    "#                  \n",
    "                min_loss_row = log_df.sort_values(by='loss',ascending=True)[0:1].to_dict(orient='records')[0]\n",
    "                 \n",
    "                loss = min_loss_row['loss']\n",
    "                mape = min_loss_row['mape']\n",
    "                val_loss = min_loss_row['val_loss']\n",
    "                val_mape = min_loss_row['val_mape']\n",
    "               \n",
    "                _mem_day = int(min_loss_row['the_mem_days'])\n",
    "                _lstm_layer = int(min_loss_row['the_lstm_layers'])\n",
    "                _dense_layer = int(min_loss_row['the_dense_layers'])\n",
    "                _unit = int(min_loss_row['the_units'])\n",
    "                save_model = fit_model\n",
    " \n",
    "                # 把不符合标准的模型从csv和文件列表中删除\n",
    "                model_saved_log2 = f'{model_saved_file}/{klt}/{ model_saved_log_char}_models.csv'\n",
    "                save_model_csv = pd.read_csv(model_saved_log2)\n",
    "            \n",
    "                #code被解析为int，再文件保存时，加上字符c保证解析为code\n",
    "                min_loss = save_model_csv.loc[save_model_csv['code'] == 'c'+code].sort_values('loss',ascending=True)[0:1].to_dict(orient='records')[0]['loss']\n",
    "                rows = save_model_csv.loc[(save_model_csv['code'] == 'c'+code )& (save_model_csv['loss'] > min_loss)]\n",
    "                for row in rows.to_dict(orient='records'):\n",
    "                    \n",
    "                    filename = row['modelname']\n",
    "                    if os.path.exists(filename):\n",
    "                        os.remove(filename)\n",
    "\n",
    "                save_model_csv = save_model_csv.drop(rows.index)\n",
    "                save_model_csv.to_csv(model_saved_log2, index=False)\n",
    "                print(f'thread{thread_count},{index},{code}:save_model_csv_{model_saved_log2}')\n",
    "                del log_df,save_model_csv\n",
    "                gc.collect()\n",
    "            else:\n",
    "                print(f'code:{code};not exit')\n",
    " \n",
    "\n",
    "    except Exception as reason:\n",
    "        print(f'-----------------Exception-----------------')\n",
    "        if reason != '超时':\n",
    "            print(f'Exception:thread{thread_count},{index}:{str(reason)}')\n",
    "            exception_file = open(exception_file_full_name, 'a')\n",
    "\n",
    "            # 写表头code,loss,mape,val_loss,val_mape,modelname\n",
    "            exception_log = f'\\'{code}\\':{reason}\\n'\n",
    "            exception_file.write(model_log)\n",
    "            exception_file.close()\n",
    "\n",
    "            lstm_model_fit(index+1,files,thread_count,klt)\n",
    "        else:\n",
    "            print(str(reason))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22f36168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.745957Z",
     "iopub.status.busy": "2023-03-12T03:38:07.745515Z",
     "iopub.status.idle": "2023-03-12T03:38:07.754331Z",
     "shell.execute_reply": "2023-03-12T03:38:07.753166Z"
    },
    "papermill": {
     "duration": 0.018016,
     "end_time": "2023-03-12T03:38:07.756828",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.738812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 时间测试\n",
    " \n",
    "_mem_days=[3] #滑动区间，根据几天的数据做预测\n",
    "_lstm_layers,_dense_layers=[1],[1] #图层数\n",
    "_units= [64]\n",
    "\n",
    "batch_size=10 #每次训练在训练集中取batchsize个样本训练；.batch_size=1时为在线学习，也是标准的SGD,如果数据集比较小，则完全可以采用全数据集的形式;GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优\n",
    "epochs=100 #一个 epoch（代）是指整个数据集正向反向训练一次。\n",
    "\n",
    "\n",
    "model_verbose = 0\n",
    "\n",
    "_time_limit = 10\n",
    " \n",
    "_time_start = time.time()\n",
    " \n",
    "\n",
    "# lstm_model_fit(0,stocks_all[_file_begin:_file_end],0,101)\n",
    "\n",
    "stock_list = stocks_all+bankuai\n",
    "\n",
    "for ep in [25,50,100]:\n",
    "    for batch  in [10,20,50,100,150,200]:\n",
    "        batch_size = batch #batch_size 越小，训练时间越长。\n",
    "        epochs = ep #epochs越小，训练时间越少\n",
    "\n",
    "        _time_start = time.time()\n",
    "#         print(f'本轮开始时间：{_time_start},batch_size：{batch},epochs:{ep} ')\n",
    "\n",
    "#         lstm_model_fit(0,stock_list[0:2],1,101)\n",
    " \n",
    "        comp_time = datetime.datetime.now()\n",
    "        _time_end = time.time()\n",
    "\n",
    "#         print(f'完成时间：{comp_time},用时：{(_time_end-_time_start)/60} min|||batch_size：{batch},epochs:{ep} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3215c130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.768829Z",
     "iopub.status.busy": "2023-03-12T03:38:07.768418Z",
     "iopub.status.idle": "2023-03-12T03:38:07.778263Z",
     "shell.execute_reply": "2023-03-12T03:38:07.776836Z"
    },
    "papermill": {
     "duration": 0.019351,
     "end_time": "2023-03-12T03:38:07.781131",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.761780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件范围0-99999\n",
      "完成时间：2023-03-12 03:38:07.773289,用时：0.00022292137145996094 s\n"
     ]
    }
   ],
   "source": [
    "# _data =lstm_cleanm_data( '/kaggle/input/stocks-data-20221216/301089.gzip')\n",
    "# ttt = build_models(_data.copy(deep=True),'301089',_mem_days,_lstm_layers,_dense_layers,_units,True,True,0)\n",
    "# #测试\n",
    "_mem_days=[3] #滑动区间，根据几天的数据做预测\n",
    "_lstm_layers,_dense_layers=[1],[1] #图层数\n",
    "_units= [64]\n",
    "\n",
    "batch_size=150 #每次训练在训练集中取batchsize个样本训练；.batch_size=1时为在线学习，也是标准的SGD,如果数据集比较小，则完全可以采用全数据集的形式;GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优\n",
    "epochs=50 #一个 epoch（代）是指整个数据集正向反向训练一次。\n",
    "\n",
    "\n",
    "model_verbose = 0\n",
    "\n",
    "_time_limit = 10\n",
    "# print(str(_lstm_layers))\n",
    "_time_start = time.time()\n",
    "\n",
    "_file_begin = 0\n",
    "_file_end = 99999\n",
    "#stocks_all,bankuai\n",
    "# files = os.listdir(path)\n",
    "print(f'文件范围{_file_begin}-{_file_end}')\n",
    "\n",
    "# lstm_model_fit(0,stocks_all[_file_begin:_file_end],0,101)\n",
    "\n",
    "# stock_list = stocks_all+bankuai\n",
    "\n",
    "\n",
    "# lock = threading.Lock()\n",
    "\n",
    "# t1 = threading.Thread(target=lstm_model_fit, args=(80,stock_list[_file_begin:_file_end],1,101))\n",
    "# t1.start()\n",
    "# threads.append(t1)\n",
    "\n",
    "# t2 = threading.Thread(target=lstm_model_fit, args=(94,stock_list[_file_begin:_file_end],2,102))\n",
    "# t2.start()\n",
    "# threads.append(t2)\n",
    "\n",
    "# t3 = threading.Thread(target=lstm_model_fit, args=(115,stock_list[_file_begin:_file_end],3,103))\n",
    "# t3.start()\n",
    "# threads.append(t3)\n",
    "\n",
    "# for thread in threads:\n",
    "#     thread.join()\n",
    "\n",
    "comp_time = datetime.datetime.now()\n",
    "_time_end = time.time()\n",
    "\n",
    "print(f'完成时间：{comp_time},用时：{_time_end-_time_start} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6cf1e9",
   "metadata": {
    "papermill": {
     "duration": 0.004603,
     "end_time": "2023-03-12T03:38:07.790790",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.786187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "480c3792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.802834Z",
     "iopub.status.busy": "2023-03-12T03:38:07.802389Z",
     "iopub.status.idle": "2023-03-12T03:38:07.811741Z",
     "shell.execute_reply": "2023-03-12T03:38:07.809832Z"
    },
    "papermill": {
     "duration": 0.019431,
     "end_time": "2023-03-12T03:38:07.815426",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.795995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/stocks.py\n",
      "/kaggle/working/__notebook__.ipynb\n",
      "/kaggle/working/models_2/20230312Mar031678592287_exception.txt\n",
      "/kaggle/working/models_2/102/20230312Mar031678592287_models.csv\n",
      "/kaggle/working/models_2/101/20230312Mar031678592287_models.csv\n",
      "/kaggle/working/models_2/103/20230312Mar031678592287_models.csv\n",
      "/kaggle/working/__pycache__/stocks.cpython-37.pyc\n"
     ]
    }
   ],
   "source": [
    "def scanfiles():\n",
    "    # 使用 os.walk 函数遍历目录及其下所有文件和子目录\n",
    "    for root, dirs, files in os.walk('/kaggle/working/'):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(file_path)\n",
    "scanfiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c98b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T03:38:07.827941Z",
     "iopub.status.busy": "2023-03-12T03:38:07.827499Z",
     "iopub.status.idle": "2023-03-12T03:38:07.833540Z",
     "shell.execute_reply": "2023-03-12T03:38:07.831926Z"
    },
    "papermill": {
     "duration": 0.015356,
     "end_time": "2023-03-12T03:38:07.836193",
     "exception": false,
     "start_time": "2023-03-12T03:38:07.820837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "# 定义要删除的目录路径\n",
    "dir_path = '/kaggle/working/'\n",
    "\n",
    "# 删除目录及其下所有文件和子目录\n",
    "# shutil.rmtree(dir_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24.732095,
   "end_time": "2023-03-12T03:38:11.362080",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-12T03:37:46.629985",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
