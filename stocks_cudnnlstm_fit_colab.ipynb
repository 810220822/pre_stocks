{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/810220822/pre_stocks/blob/main/stocks_cudnnlstm_fit_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 对完成处理的股票交易数据（分组、归一化）做lstm建模\n",
        "输入：处理好的数据\n",
        "输出：模型和模型评价"
      ],
      "metadata": {
        "id": "2jqx-A00hlFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "l3Ox1iGDhug0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b03995-d7fc-4c22-9ce8-112d866103aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 从kaggle下载数据到项目"
      ],
      "metadata": {
        "id": "dp7I4vCnLMQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "6JTLQTakh9w4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3af91c-50ae-4df9-b220-15b8e8b44725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.14)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "token = {\"username\":\"qianqian2022\",\"key\":\"1425dc4c54e08eeea5bfeae5624aeecb\"} #上面中token字典中键对应的值是你账号的内容，具体操作如下：\n",
        "with open('/content/kaggle.json', 'w') as file:\n",
        "  json.dump(token, file)\n"
      ],
      "metadata": {
        "id": "9XCupvariF7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "!kaggle config set -n path -v /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SJypRyDiKKy",
        "outputId": "5900aaad-2ab2-4b2e-8929-09933a5d4df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- path is now set to: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d qianqian2022/stock-prefit-0306 -p /content/stock/data/working"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2lrsC2PiPcU",
        "outputId": "cfa27e10-4347-4d7d-c2dd-f21243605c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading stock-prefit-0306.zip to /content/stock/data/working\n",
            " 99% 945M/959M [00:10<00:00, 147MB/s]\n",
            "100% 959M/959M [00:10<00:00, 99.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o /content/stock/data/working/stock-prefit-0306.zip -d /content/drive/MyDrive/stock/data"
      ],
      "metadata": {
        "id": "7X6jzFuSiQs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 模型训练"
      ],
      "metadata": {
        "id": "REdptUrULaK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import datetime\n",
        "from sklearn.preprocessing import StandardScaler # pip3 install --upgrade --force-reinstall scikit-learn --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential #pip3 install --upgrade --force-reinstall keras --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
        "from tensorflow.keras.models import load_model #pip3 install --upgrade --force-reinstall keras --target . -i https://pypi.mirrors.ustc.edu.cn/simple\n",
        "from tensorflow.keras.layers import LSTM,Dense,Dropout\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,Callback,CSVLogger,ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from keras.utils import multi_gpu_utils\n",
        "import os\n",
        "from io import StringIO\n",
        "import gzip\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import time\n",
        "import gc \n",
        "from shutil import copyfile\n",
        "# copy our file into the working directory (make sure it has .py suffix)\n",
        "shutil.copyfile(\"/content/drive/MyDrive/stock/data/stocks.py\", './stocks.py')\n",
        "# copyfile(src = \"/content/stock/data/stocks.py\", dst = \"/content/stock/data\")\n",
        " \n",
        "# import all our functions\n",
        "from stocks import stocks_all\n",
        "from stocks import bankuai\n",
        "\n",
        "import pickle\n",
        "\n",
        "import threading\n",
        "from queue import Queue\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #消除tensorflow警告\n",
        "\n",
        "model_saved_log_char = datetime.datetime.now().strftime('%Y%m%d%h%m%s')\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-03-11T10:08:23.259517Z",
          "iopub.execute_input": "2023-03-11T10:08:23.260564Z",
          "iopub.status.idle": "2023-03-11T10:08:23.395319Z",
          "shell.execute_reply.started": "2023-03-11T10:08:23.260522Z",
          "shell.execute_reply": "2023-03-11T10:08:23.394216Z"
        },
        "trusted": true,
        "id": "12_P9xaPhlFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#获取数据\n",
        "start = datetime.datetime(2000,1,1)\n",
        "end =  datetime.date.today()\n",
        "\n",
        "#参数整理\n",
        "EarlyStopping_monitor='val_loss' #monitor——被监测的量\n",
        "EarlyStopping_patience=10 #检测值停止变化的次数\n",
        "\n",
        "_mem_days=[1,3,5] #滑动区间，根据几天的数据做预测\n",
        "_lstm_layers,_dense_layers=[1,5],[1,5] #图层数\n",
        "# 这里我们设置的units=32的大小，其实代表得是LSTM单元内的隐藏层的尺寸。\n",
        "# 对于LSTM而言，每个单元有3个门，对应了4个激活函数（3个sigmoid,一个tanh）。也就是说有4个神经元数量为32的前馈网络层。\n",
        "_units= [32,64]\n",
        "\n",
        "# #测试\n",
        "# _mem_days=[3] #滑动区间，根据几天的数据做预测\n",
        "# _lstm_layers,_dense_layers=[1],[1] #图层数\n",
        "# _units= [32]\n",
        "\n",
        "\n",
        "optimizer='adam' #优化器:控制梯度下降和梯度爆炸\n",
        "loss = 'mse' #损失层\n",
        "metrics=['mape'] #评价函数\n",
        "batch_size=32 #每次训练在训练集中取batchsize个样本训练；.batch_size=1时为在线学习，也是标准的SGD,如果数据集比较小，则完全可以采用全数据集的形式;GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优\n",
        "epochs=50 #一个 epoch（代）是指整个数据集正向反向训练一次。\n",
        "\n",
        "model_verbose = 0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-11T10:08:27.713460Z",
          "iopub.execute_input": "2023-03-11T10:08:27.714321Z",
          "iopub.status.idle": "2023-03-11T10:08:27.896277Z",
          "shell.execute_reply.started": "2023-03-11T10:08:27.714287Z",
          "shell.execute_reply": "2023-03-11T10:08:27.895441Z"
        },
        "trusted": true,
        "id": "q2E7XP0NhlFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#文件路径 data_\n",
        "# path = '/kaggle/input/stocks-data-20221216/'\n",
        "log_file_name = '/content/drive/MyDrive/stock/data/working/models'\n",
        "model_saved_file='/content/drive/MyDrive/stock/data/working/models_2'\n",
        "INPUT_PATH = '/content/drive/MyDrive/stock/data/data_pre_fit'\n",
        "\n",
        "\n",
        "\n",
        "[os.makedirs(f\"{log_file_name}/{klt}\", exist_ok=True) for klt in [101, 102, 103]]\n",
        "[os.makedirs(f\"{model_saved_file}/{klt}\", exist_ok=True) for klt in [101, 102, 103]]\n",
        "\n",
        "for _klt_ in [101,102,103]:\n",
        "    model_saved_log = f'{model_saved_file}/{_klt_}/{ model_saved_log_char}_models.csv'\n",
        "    # #创建任务总模型目录\n",
        "    log_csv_file = open(model_saved_log, 'a')\n",
        "\n",
        "    # 写表头code,loss,mape,val_loss,val_mape,modelname\n",
        "    model_log = f'code,klt,loss,mape,val_loss,val_mape,modelname\\n'\n",
        "    log_csv_file.write(model_log)\n",
        "    log_csv_file.close()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-11T10:08:31.058674Z",
          "iopub.execute_input": "2023-03-11T10:08:31.059597Z",
          "iopub.status.idle": "2023-03-11T10:08:31.067309Z",
          "shell.execute_reply.started": "2023-03-11T10:08:31.059560Z",
          "shell.execute_reply": "2023-03-11T10:08:31.066559Z"
        },
        "trusted": true,
        "id": "nFtdTRa4hlFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exception_file_full_name = f'{model_saved_file}/{ model_saved_log_char}_exception.txt'\n",
        "\n",
        "#创建异常文件\n",
        "exception_file = open(exception_file_full_name, 'a')\n",
        "\n",
        "# 写表头code,loss,mape,val_loss,val_mape,modelname\n",
        "exception_log = f'---------------Exception:{str(end)}------------------\\n'\n",
        "exception_file.write(exception_log)\n",
        "exception_file.close()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-11T10:08:34.362066Z",
          "iopub.execute_input": "2023-03-11T10:08:34.362459Z",
          "iopub.status.idle": "2023-03-11T10:08:34.367777Z",
          "shell.execute_reply.started": "2023-03-11T10:08:34.362427Z",
          "shell.execute_reply": "2023-03-11T10:08:34.366994Z"
        },
        "trusted": true,
        "id": "3hs-Yxr4hlFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CustomCallback 类"
      ],
      "metadata": {
        "id": "69S5zjWHLiVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#模型callback类\n",
        "class CustomCallback(Callback):\n",
        "#     print('-----------------CustomCallback-----------------')\n",
        "    code = ''\n",
        "    the_mem_days=0\n",
        "    the_lstm_layers=0\n",
        "    the_dense_layers=0\n",
        "    the_units = 0\n",
        "    csv_file_name = ''\n",
        "    model_path = ''\n",
        "    saveModelFile = False\n",
        "    saveModelLog = True\n",
        "\n",
        "    #epoch,loss,mape,val_loss,val_mape,code,the_mem_days,the_lstm_layers,the_dense_layers,the_units\n",
        "    csv_file = DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,path,csv_file_name,code,the_mem_days,the_lstm_layers,the_dense_layers,the_units,\n",
        "                 saveModelFile=False,saveModelLog=True,klt=101):\n",
        "#         print(f'-----------------path:{path},klt:{klt},code:{code}-----------------\\n')\n",
        "        self.model_path = path\n",
        "        self.csv_file_name = csv_file_name\n",
        "        self.code = code\n",
        "        self.the_mem_days = the_mem_days\n",
        "        self.the_lstm_layers = the_lstm_layers\n",
        "        self.the_dense_layers = the_dense_layers\n",
        "        self.the_units = the_units\n",
        "        self.saveModelFile = saveModelFile\n",
        "        self.saveModelLog=saveModelLog\n",
        "        self.klt = klt\n",
        "#         print(f'-----------------CustomCallback__init__,klt:{klt},code:{code}-----------------\\n')\n",
        "        #\n",
        "        if not os.path.exists(csv_file_name):\n",
        "#             print(f'-----------------os.path.exists(csv_file_name),klt:{klt},code:{code}-----------------\\n')\n",
        "            # #创建任务总模型目录\n",
        "            _temp_file = open(csv_file_name, 'a') \n",
        "            _temp_file_header = f'epoch,loss,mape,val_loss,val_mape,code,klt,the_mem_days,the_lstm_layers,the_dense_layers,the_units\\n'\n",
        "#             print(f'-----------------_temp_file_header:{_temp_file_header},klt:{klt},code:{code}-----------------\\n')\n",
        "            _temp_file.write(_temp_file_header)\n",
        "            _temp_file.close()\n",
        "#         print(f'-----------------self.csv_file,klt:{klt},code:{code}-----------------\\n')\n",
        "        self.csv_file = pd.read_csv(csv_file_name, lineterminator='\\n', header=0)  \n",
        "                \n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "#         print(f'-----------------self.on_epoch_end,klt:{self.klt},code:{self.code}-----------------\\n')\n",
        "        if self.saveModelFile == True:\n",
        "#             print(f'-----------------self.saveModelFile,klt:{self.klt},code:{self.code}-----------------\\n')\n",
        "            loss = logs['loss']\n",
        "            filepath =  f'{self.model_path}/{loss:.2f}_{self.code}_{epoch:02}_mem_{self.the_mem_days}_ltsm_{self.the_lstm_layers}_dense_{self.the_dense_layers}_unit_{self.the_units}.h5'\n",
        "#             print(f'-----------------filepath:{filepath}-----------------\\n')\n",
        "            loss = logs['loss']\n",
        "            mape = logs['mape']\n",
        "            val_loss = logs['val_loss']\n",
        "            val_mape = logs['val_mape']\n",
        "            model_saved_log1 = f'{model_saved_file}/{self.klt}/{ model_saved_log_char}_models.csv'\n",
        "#             print(f'-----------------model_saved_log:{model_saved_log1},klt:{self.klt},code:{self.code}-----------------\\n')\n",
        "            log_csv_file = open(model_saved_log1, 'a+')\n",
        "            # code,loss,mape,val_loss,val_mape,modelname\n",
        "            model_log = f'c{self.code},{self.klt},{loss:.2f},{mape:.2f},{val_loss:.2f},{val_mape:.2f},{filepath}\\n'\n",
        "            log_csv_file.write(model_log)\n",
        "            log_csv_file.close()\n",
        "            \n",
        "#             print(f'-----------------filepath:{filepath},klt:{self.klt},code:{self.code}-----------------\\n')\n",
        "\n",
        "            self.model.save(filepath,save_format='h5')\n",
        "#         print(f'-----------------self.saveModelLog:{self.saveModelLog},klt:{self.klt},code:{self.code}-----------------\\n')\n",
        "        if self.saveModelLog == True:\n",
        "#             print(f'-----------------logs.loss{logs['loss']}-----------------\\n')\n",
        "            if not math.isnan(logs['loss']) :\n",
        "#                 print(f'-----------------self.csv_file{self.csv_file},klt:{klt},code:{code}-----------------\\n')\n",
        "                _i_ = len(self.csv_file)\n",
        "#                 print(f'-----------------_i_:{_i_},klt:{self.klt},code:{self.code}-----------------\\n')\n",
        "                row = {\n",
        "                    'epoch':epoch,\n",
        "                    'loss' : float(round(logs['loss'],2) ),\n",
        "                    'mape':round(logs['mape'],2)  ,\n",
        "                    'val_loss': round(logs['val_loss'],2) ,\n",
        "                    'val_mape': round(logs['val_mape'],2)  ,\n",
        "\n",
        "                    'code': self.code,\n",
        "                    'klt':self.klt,\n",
        "                    'the_mem_days': self.the_mem_days,\n",
        "                    'the_lstm_layers': self.the_lstm_layers,\n",
        "                    'the_dense_layers': self.the_dense_layers,\n",
        "                    'the_units': self.the_units\n",
        "                }\n",
        "#                 print(f'-----------------row{row},klt:{self.klt},code:{self.code}-----------------\\n')\n",
        "                row_index = len(self.csv_file)\n",
        "#                 print(f'-----------------self.csv_file.loc:{313},klt:{self.klt},code:{self.code}-----------------\\n')\n",
        "                self.csv_file.loc[row_index] = row\n",
        "#                 print(f'-----------------{self.csv_file_name},klt:{self.klt},code:{self.code}-----------------\\n')\n",
        "                self.csv_file.to_csv(self.csv_file_name,index=False)\n",
        " "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-11T10:08:35.930208Z",
          "iopub.execute_input": "2023-03-11T10:08:35.930928Z",
          "iopub.status.idle": "2023-03-11T10:08:35.951484Z",
          "shell.execute_reply.started": "2023-03-11T10:08:35.930894Z",
          "shell.execute_reply": "2023-03-11T10:08:35.950676Z"
        },
        "trusted": true,
        "id": "lblMh2anhlFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## build_models"
      ],
      "metadata": {
        "id": "iMmiyWMaMfbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#建模\n",
        "def build_models(file_path,code,mem_days,lstm_layers,dense_layers,units,saveModelFile ,saveModelLog,thread_count,klt ):\n",
        "    \n",
        "    build_models_times = 0\n",
        "\n",
        "    for the_mem_days in mem_days:\n",
        "#         new_df = f\n",
        "        x, y = open_data_processd(file_path,klt)\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y, shuffle=False, test_size=0.2, random_state=42)\n",
        "        \n",
        "        # 转换为 Dataset 对象\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "\n",
        "        for the_lstm_layers in lstm_layers:\n",
        "            for the_dense_layers in dense_layers:\n",
        "                for the_units in units:\n",
        "#                     print('-----------------callback-----------------')\n",
        "                    callback = [\n",
        "                        EarlyStopping(monitor=EarlyStopping_monitor, patience=EarlyStopping_patience),\n",
        "                        # CSVLogger(filename, separator=',', append=True),\n",
        "                        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto',\n",
        "                                          min_delta=0.0001, cooldown=0, min_lr=0),\n",
        "                        CustomCallback(f'{model_saved_file}/{klt}',f'{log_file_name}/{klt}/{code}.csv',code,the_mem_days,the_lstm_layers,the_dense_layers,the_units,\n",
        "                                       saveModelFile=saveModelFile,saveModelLog=saveModelLog,klt=klt)\n",
        "                    ]\n",
        "#                     print('-----------------Sequential-----------------')\n",
        "                    #构建神经网络\n",
        "                    model = Sequential()\n",
        "#                     from keras.utils import multi_gpu_utils\n",
        "#                     model = multi_gpu_utils(model, gpus=2)\n",
        "                    model.add(CuDNNLSTM(the_units,input_shape=x.shape[1:],return_sequences=True)) #第一层\n",
        "                    model.add(Dropout(0.1)) #防止过拟合\n",
        "\n",
        "                    for i in range(the_lstm_layers):\n",
        "                        model.add(CuDNNLSTM(the_units,return_sequences=True)) #要有返回值\n",
        "                        model.add(Dropout(0.1)) #防止过拟合\n",
        "\n",
        "                    model.add(CuDNNLSTM(the_units))\n",
        "                    model.add(Dropout(0.1)) #防止过拟合\n",
        "\n",
        "                    for i in range(the_dense_layers):\n",
        "                        model.add(Dense(the_units,activation='relu'))  #全连接层\n",
        "                        model.add(Dropout(0.1)) #防止过拟合\n",
        "\n",
        "                    model.add(Dense(1)) #输出层\n",
        "\n",
        "                    model.compile(optimizer='adam' ,#优化器\n",
        "                                  loss = 'mse' ,#损失层\n",
        "                                  metrics=['mape'])#评价函数) #编译\n",
        "\n",
        "                    print(f'thread{thread_count},{code},NO.{build_models_times}:{the_mem_days},{the_lstm_layers},{the_dense_layers},{the_units},{str(datetime.datetime.now())}')\n",
        "                    # print(f'batch_size:{batch_size},epochs:{epochs}')\n",
        "#                     model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test,y_test),verbose=model_verbose,callbacks=callback)\n",
        "                    model.fit(train_dataset.prefetch(tf.data.experimental.AUTOTUNE),epochs=epochs,\n",
        "                              validation_data=(x_test,y_test),verbose=model_verbose,callbacks=callback)\n",
        "                   \n",
        "                    del train_dataset,x_test,x_train,y_test,y_train\n",
        "#                     print('-----------------build_models_times-----------------')\n",
        "                    build_models_times+=1\n",
        "    return build_models_times"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-11T10:35:22.994420Z",
          "iopub.execute_input": "2023-03-11T10:35:22.995235Z",
          "iopub.status.idle": "2023-03-11T10:35:23.014003Z",
          "shell.execute_reply.started": "2023-03-11T10:35:22.995196Z",
          "shell.execute_reply": "2023-03-11T10:35:23.013139Z"
        },
        "trusted": true,
        "id": "VWS0ZtV2hlFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lstm_model_fit"
      ],
      "metadata": {
        "id": "2JTUgkORMmus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_model_fit(begin,files,thread_count,klt):\n",
        "    thread_count = thread_count\n",
        "    try:\n",
        "        \n",
        "        for index in range(begin,len(files)):\n",
        "            #超时打包\n",
        "            if time.time()-_time_start >_time_limit*60*60 :\n",
        "              print('time out')\n",
        "              return\n",
        "\n",
        "            if os.path.exists('/content/stop'):\n",
        "              #退出循环\n",
        "              break\n",
        "            \n",
        "            code = files[index]\n",
        "            file_path = f'{INPUT_PATH}/{klt}/{code}.pkl'\n",
        "          \n",
        "            if os.path.exists(file_path):\n",
        " \n",
        "                fit_model = build_models(file_path,code,_mem_days,_lstm_layers,_dense_layers,_units,True,True,f'{thread_count},{index}',klt)\n",
        "                \n",
        "                 \n",
        "                log_df = pd.read_csv(f'{log_file_name}/{klt}/{code}.csv', lineterminator='\\n', header=0)\n",
        "\n",
        "                min_loss_row = log_df.sort_values(by='loss',ascending=True)[0:1].to_dict(orient='records')[0]\n",
        " \n",
        "                loss = min_loss_row['loss']\n",
        "                mape = min_loss_row['mape']\n",
        "                val_loss = min_loss_row['val_loss']\n",
        "                val_mape = min_loss_row['val_mape']\n",
        " \n",
        "                _mem_day = int(min_loss_row['the_mem_days'])\n",
        "                _lstm_layer = int(min_loss_row['the_lstm_layers'])\n",
        "                _dense_layer = int(min_loss_row['the_dense_layers'])\n",
        "                _unit = int(min_loss_row['the_units'])\n",
        "                save_model = fit_model\n",
        "\n",
        "                model_saved_log2 = f'{model_saved_file}/{klt}/{ model_saved_log_char}_models.csv'\n",
        "                save_model_csv = pd.read_csv(model_saved_log2)\n",
        "                #code被解析为int，再文件保存时，加上字符c保证解析为code\n",
        "                min_loss = save_model_csv.loc[save_model_csv['code'] == 'c'+code].sort_values('loss',ascending=True)[0:1].to_dict(orient='records')[0]['loss']\n",
        "                rows = save_model_csv.loc[(save_model_csv['code'] == 'c'+code )& (save_model_csv['loss'] > min_loss)]\n",
        "                for row in rows.to_dict(orient='records'):\n",
        "                    \n",
        "                    filename = row['modelname']\n",
        "                    if os.path.exists(filename):\n",
        "                        os.remove(filename)\n",
        "\n",
        "                save_model_csv = save_model_csv.drop(rows.index)\n",
        "                save_model_csv.to_csv(model_saved_log2, index=False)\n",
        "                print(f'thread{thread_count},{index},{code}:save_model_csv_{model_saved_log2}')\n",
        "\n",
        "                del log_df,save_model_csv\n",
        "                gc.collect()\n",
        "            else:\n",
        "                print(f'code:{code};not exit')\n",
        "#      \n",
        "\n",
        "    except Exception as reason:\n",
        "        print(f'-----------------Exception-----------------')\n",
        "        if reason != '超时':\n",
        "            print(f'Exception:thread{thread_count},{index}:{str(reason)}')\n",
        "            exception_file = open(exception_file_full_name, 'a')\n",
        "\n",
        "            # 写表头code,loss,mape,val_loss,val_mape,modelname\n",
        "            exception_log = f'\\'{code}\\':{reason}\\n'\n",
        "            exception_file.write(model_log)\n",
        "            exception_file.close()\n",
        "\n",
        "            lstm_model_fit(index+1,files,thread_count,klt)\n",
        "        else:\n",
        "            print(str(reason))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-11T10:08:49.167857Z",
          "iopub.execute_input": "2023-03-11T10:08:49.168616Z",
          "iopub.status.idle": "2023-03-11T10:08:49.187223Z",
          "shell.execute_reply.started": "2023-03-11T10:08:49.168582Z",
          "shell.execute_reply": "2023-03-11T10:08:49.186490Z"
        },
        "trusted": true,
        "id": "ZJttUj-6hlFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 公共方法"
      ],
      "metadata": {
        "id": "J37ZTyUCMvx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def open_data_processd(file_path,klt):\n",
        "    \n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        x, y = pickle.load(f)\n",
        "        \n",
        "    return x, y\n",
        "       "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-11T10:08:47.397164Z",
          "iopub.execute_input": "2023-03-11T10:08:47.397814Z",
          "iopub.status.idle": "2023-03-11T10:08:47.402208Z",
          "shell.execute_reply.started": "2023-03-11T10:08:47.397780Z",
          "shell.execute_reply": "2023-03-11T10:08:47.401461Z"
        },
        "trusted": true,
        "id": "Nt719o_xhlFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 用于时长测试"
      ],
      "metadata": {
        "id": "PHxOd6mGNPOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 时间测试\n",
        " \n",
        "_mem_days=[3] #滑动区间，根据几天的数据做预测\n",
        "_lstm_layers,_dense_layers=[1],[1] #图层数\n",
        "_units= [64]\n",
        "\n",
        "batch_size=10 #每次训练在训练集中取batchsize个样本训练；.batch_size=1时为在线学习，也是标准的SGD,如果数据集比较小，则完全可以采用全数据集的形式;GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优\n",
        "epochs=100 #一个 epoch（代）是指整个数据集正向反向训练一次。\n",
        "\n",
        "\n",
        "model_verbose = 0\n",
        "\n",
        "_time_limit = 10\n",
        " \n",
        "_time_start = time.time()\n",
        " \n",
        "\n",
        "# lstm_model_fit(0,stocks_all[_file_begin:_file_end],0,101)\n",
        "\n",
        "stock_list = stocks_all+bankuai\n",
        "\n",
        "for ep in [25,50,100]:\n",
        "    for batch  in [10,20,50,100,150,200]:\n",
        "        batch_size = batch #batch_size 越小，训练时间越长。\n",
        "        epochs = ep #epochs越小，训练时间越少\n",
        "\n",
        "        _time_start = time.time()\n",
        "        print(f'本轮开始时间：{_time_start},batch_size：{batch},epochs:{ep} ')\n",
        "\n",
        "        # lstm_model_fit(0,stock_list[0:2],1,101)\n",
        " \n",
        "        comp_time = datetime.datetime.now()\n",
        "        _time_end = time.time()\n",
        "\n",
        "        print(f'完成时间：{comp_time},用时：{(_time_end-_time_start)/60} min|||batch_size：{batch},epochs:{ep} ')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-11T10:08:57.088313Z",
          "iopub.execute_input": "2023-03-11T10:08:57.088720Z",
          "iopub.status.idle": "2023-03-11T10:09:09.015057Z",
          "shell.execute_reply.started": "2023-03-11T10:08:57.088675Z",
          "shell.execute_reply": "2023-03-11T10:09:09.013204Z"
        },
        "trusted": true,
        "id": "fGPVcwu_hlFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 用于正式模型训练"
      ],
      "metadata": {
        "id": "kJXl4AtfNZBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# _data =lstm_cleanm_data( '/kaggle/input/stocks-data-20221216/301089.gzip')\n",
        "# ttt = build_models(_data.copy(deep=True),'301089',_mem_days,_lstm_layers,_dense_layers,_units,True,True,0)\n",
        "# #测试\n",
        "_mem_days=[3] #滑动区间，根据几天的数据做预测\n",
        "_lstm_layers,_dense_layers=[1],[1] #图层数\n",
        "_units= [64]\n",
        "\n",
        "batch_size=150 #每次训练在训练集中取batchsize个样本训练；.batch_size=1时为在线学习，也是标准的SGD,如果数据集比较小，则完全可以采用全数据集的形式;GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优\n",
        "epochs=50 #一个 epoch（代）是指整个数据集正向反向训练一次。\n",
        "\n",
        "\n",
        "model_verbose = 0\n",
        "\n",
        "_time_start = time.time()\n",
        "_time_limit = 10\n",
        "_time_limit = 10\n",
        "# print(str(_lstm_layers))\n",
        "_time_start = time.time()\n",
        "\n",
        "_file_begin = 0\n",
        "_file_end = 99999\n",
        "#stocks_all,bankuai\n",
        "# files = os.listdir(path)\n",
        "print(f'文件范围{_file_begin}-{_file_end}')\n",
        "\n",
        "# lstm_model_fit(0,stocks_all[_file_begin:_file_end],0,101)\n",
        "\n",
        "stock_list = stocks_all+bankuai\n",
        "\n",
        "threads = []\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "t1 = threading.Thread(target=lstm_model_fit, args=(77,stock_list[_file_begin:_file_end],1,101))\n",
        "t1.start()\n",
        "threads.append(t1)\n",
        "\n",
        "t2 = threading.Thread(target=lstm_model_fit, args=(90,stock_list[_file_begin:_file_end],2,102))\n",
        "t2.start()\n",
        "threads.append(t2)\n",
        "\n",
        "t3 = threading.Thread(target=lstm_model_fit, args=(110,stock_list[_file_begin:_file_end],3,103))\n",
        "t3.start()\n",
        "threads.append(t3)\n",
        "\n",
        "for thread in threads:\n",
        "  thread.join()\n",
        "\n",
        "comp_time = datetime.datetime.now()\n",
        "_time_end = time.time()\n",
        " \n",
        "\n",
        "print(f'完成时间：{comp_time},用时：{_time_end-_time_start} s')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-06T13:13:31.407779Z",
          "iopub.execute_input": "2023-03-06T13:13:31.408179Z",
          "iopub.status.idle": "2023-03-06T13:15:10.467028Z",
          "shell.execute_reply.started": "2023-03-06T13:13:31.408140Z",
          "shell.execute_reply": "2023-03-06T13:15:10.465925Z"
        },
        "trusted": true,
        "id": "BE1N4UFxhlFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 用于线程测试"
      ],
      "metadata": {
        "id": "vkJisr5C13KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# _data =lstm_cleanm_data( '/kaggle/input/stocks-data-20221216/301089.gzip')\n",
        "# ttt = build_models(_data.copy(deep=True),'301089',_mem_days,_lstm_layers,_dense_layers,_units,True,True,0)\n",
        "# #测试\n",
        "_mem_days=[3] #滑动区间，根据几天的数据做预测\n",
        "_lstm_layers,_dense_layers=[1],[1] #图层数\n",
        "_units= [64]\n",
        "\n",
        "batch_size=150 #每次训练在训练集中取batchsize个样本训练；.batch_size=1时为在线学习，也是标准的SGD,如果数据集比较小，则完全可以采用全数据集的形式;GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优\n",
        "epochs=50 #一个 epoch（代）是指整个数据集正向反向训练一次。\n",
        "\n",
        "\n",
        "model_verbose = 0\n",
        "\n",
        "_time_start = time.time()\n",
        "_time_limit = 10\n",
        "_time_limit = 10\n",
        "# print(str(_lstm_layers))\n",
        "_time_start = time.time()\n",
        "\n",
        "_file_begin = 0\n",
        "_file_end = 99999\n",
        "#stocks_all,bankuai\n",
        "# files = os.listdir(path)\n",
        "print(f'文件范围{_file_begin}-{_file_end}')\n",
        "\n",
        "# lstm_model_fit(0,stocks_all[_file_begin:_file_end],0,101)\n",
        "\n",
        "stock_list = stocks_all+bankuai\n",
        "\n",
        "threads = []\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "t1 = threading.Thread(target=lstm_model_fit, args=(77,stock_list[_file_begin:_file_end],1,101))\n",
        "t1.start()\n",
        "threads.append(t1)\n",
        "\n",
        "t2 = threading.Thread(target=lstm_model_fit, args=(90,stock_list[_file_begin:_file_end],2,102))\n",
        "t2.start()\n",
        "threads.append(t2)\n",
        "\n",
        "t3 = threading.Thread(target=lstm_model_fit, args=(110,stock_list[_file_begin:_file_end],3,103))\n",
        "t3.start()\n",
        "threads.append(t3)\n",
        "\n",
        "for thread in threads:\n",
        "  thread.join()\n",
        "\n",
        "\n",
        "for temp in stock_list:\n",
        "  for i in range(6):\n",
        "    t1 = threading.Thread(target=lstm_model_fit, args=(77,stock_list[_file_begin:_file_end],1,101))\n",
        "    t1.start()\n",
        "    threads.append(t1)\n",
        "    \n",
        "for thread in threads:\n",
        "  thread.join()\n",
        "\n",
        "comp_time = datetime.datetime.now()\n",
        "_time_end = time.time()\n",
        " \n",
        "\n",
        "print(f'完成时间：{comp_time},用时：{_time_end-_time_start} s')"
      ],
      "metadata": {
        "id": "wOgXYHkU1z0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取当前所有活动的线程列表\n",
        "threads = threading.enumerate()\n",
        "print(len(threads))\n",
        "# 遍历列表，逐个停止线程\n",
        "for t in threads:\n",
        "\n",
        "  print(t)\n",
        "  if t is threading.currentThread():\n",
        "      # 跳过当前线程\n",
        "      # print(t)\n",
        "      continue\n",
        "  # t.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xWSaj5fdmUw",
        "outputId": "1b788dc1-d382-41de-d4a3-36cbbedaa33b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "<_MainThread(MainThread, started 140518789736256)>\n",
            "<Thread(Thread-2, started daemon 140518711305984)>\n",
            "<Heartbeat(Thread-3, started daemon 140518702913280)>\n",
            "<ParentPollerUnix(Thread-1, started daemon 140518282405632)>\n",
            "<Thread(_colab_inspector_thread, started daemon 140518025582336)>\n",
            "<Thread(Thread-10, started daemon 140518664976128)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 上传github"
      ],
      "metadata": {
        "id": "M7XlizIzNjx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyGithub #安装PyGithub依赖包"
      ],
      "metadata": {
        "id": "fxjacp_aLHKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efac884e-b393-415a-a692-d739485aa49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyGithub\n",
            "  Downloading PyGithub-1.58.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.4/312.4 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.9/dist-packages (from PyGithub) (2.25.1)\n",
            "Collecting pyjwt>=2.4.0\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Collecting pynacl>=1.4.0\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 KB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from pynacl>=1.4.0->PyGithub) (1.15.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.14.0->PyGithub) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.14.0->PyGithub) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.14.0->PyGithub) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.9/dist-packages (from deprecated->PyGithub) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
            "Installing collected packages: pyjwt, deprecated, pynacl, PyGithub\n",
            "Successfully installed PyGithub-1.58.0 deprecated-1.2.13 pyjwt-2.6.0 pynacl-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from github import Github"
      ],
      "metadata": {
        "id": "idTjzRZBUkdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace <ACCESS_TOKEN> with your Personal Access Token (PAT)\n",
        "access_token = \"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\"\n",
        "\n",
        "# Create a Github instance using the access token\n",
        "g2 = Github(access_token)\n",
        "\n",
        "# Test authentication by accessing the authenticated user's details\n",
        "user = g2.get_user()\n",
        " \n",
        "# 获取指定的仓库\n",
        "# repo_name = input(\"请输入要上传文件的仓库名称：\")\n",
        "repo = g2.get_user().get_repo('pre_stocks')\n"
      ],
      "metadata": {
        "id": "FxlViVpmUrDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取要上传的文件路径和文件名\n",
        "root_directory = '/content/drive/MyDrive/stock/data/working'  # 修改为你想搜索的根目录地址\n",
        "\n",
        "import io\n",
        "replaced_roots = []\n",
        "for root, dirs, files in os.walk(root_directory):\n",
        "#     print(f'root:{root},dirs:{dirs},files:{files}')\n",
        "    if 'models_2' in dirs or 'models' in dirs:\n",
        "        replaced_root = root+'/'\n",
        "        replaced_roots.append(replaced_root)\n",
        "#         print(f'replaced_root:{root}')\n",
        "\n",
        "    if 'models_2' in root or 'models' in root:\n",
        "        for file in files:\n",
        "#             print(f'file:{file}')\n",
        "            file_path = os.path.join(root, file)\n",
        "            replaced_file_path = file_path.replace(replaced_root,'')\n",
        "            dir_path = root.replace(replaced_root,'')\n",
        "            \n",
        "#             print(f'file_path:{file_path},replaced_file_path:{replaced_file_path}')\n",
        "            \n",
        "            # 读取要上传的文件内容\n",
        "            with open(file_path, 'rb') as file:\n",
        "#                 content = file.read()\n",
        "                content = io.BytesIO(file.read()).getvalue()\n",
        "\n",
        "            # 上传到的目录\n",
        "            directory = 'Models/'\n",
        "\n",
        "\n",
        "            # 创建新的文件对象并上传到GitHub仓库中\n",
        "            try:\n",
        "              repo.create_file(directory + replaced_file_path, \"upload file\", content)\n",
        "              print(f\"path:{directory + replaced_file_path}\")\n",
        "            except Exception as e:\n",
        "              print(f\"文件 {file} 上传失败，错误信息为：{e}\")\n",
        "# "
      ],
      "metadata": {
        "id": "LhSby3S2Utz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 其他"
      ],
      "metadata": {
        "id": "MhfxQlesUlFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scanfiles():\n",
        "    # 使用 os.walk 函数遍历目录及其下所有文件和子目录\n",
        "    for root, dirs, files in os.walk('/content/drive/MyDrive/stock/data/data_pre_fit/102'):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            print(file_path)\n",
        "scanfiles()\n",
        "print('d')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-06T13:15:10.468328Z",
          "iopub.execute_input": "2023-03-06T13:15:10.468640Z",
          "iopub.status.idle": "2023-03-06T13:15:10.477516Z",
          "shell.execute_reply.started": "2023-03-06T13:15:10.468587Z",
          "shell.execute_reply": "2023-03-06T13:15:10.476196Z"
        },
        "trusted": true,
        "id": "6-4LzOQjhlFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "# 定义要删除的目录路径\n",
        "dir_path = '/kaggle/working/'\n",
        "\n",
        "# 删除目录及其下所有文件和子目录\n",
        "# shutil.rmtree(dir_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-06T13:15:10.479718Z",
          "iopub.execute_input": "2023-03-06T13:15:10.480148Z",
          "iopub.status.idle": "2023-03-06T13:15:10.486078Z",
          "shell.execute_reply.started": "2023-03-06T13:15:10.480110Z",
          "shell.execute_reply": "2023-03-06T13:15:10.484893Z"
        },
        "trusted": true,
        "id": "DBXfHnhAhlFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}